[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MAT031: Probabilidad y Estadística",
    "section": "",
    "text": "Bienvenidos a MAT031!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#contenidos-del-curso",
    "href": "index.html#contenidos-del-curso",
    "title": "MAT031: Probabilidad y Estadística",
    "section": "Contenidos del Curso",
    "text": "Contenidos del Curso\n\n\n  \n    \n      \n    \n    \n      Estadística Descriptiva\n      \n        Organización, resumen y visualización de datos mediante\n        tablas, gráficos y medidas numéricas descriptivas.\n      \n    \n  \n\n  \n    \n      \n    \n    \n      Probabilidad\n      \n        Modelación de la incertidumbre a través de experimentos\n        aleatorios, variables aleatorias y distribuciones clásicas.\n      \n    \n  \n\n  \n    \n      \n    \n    \n      Inferencia Estadística\n      \n        Estimación de parámetros, intervalos de confianza y pruebas\n        de hipótesis para apoyar la toma de decisiones.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/intro.html",
    "href": "material/modulo_01/intro.html",
    "title": "Introducción",
    "section": "",
    "text": "Proceso de medición",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/intro.html#proceso-de-medición",
    "href": "material/modulo_01/intro.html#proceso-de-medición",
    "title": "Introducción",
    "section": "",
    "text": "Medir es construir una representación del mundo\nEn la vida cotidiana y en las organizaciones estamos rodeados de indicadores: porcentajes, índices, promedios, rankings. Estos números influyen en diagnósticos, evaluaciones y decisiones. Sin embargo, rara vez se cuestiona cómo se obtienen ni qué tan fielmente representan la realidad.\nLa estadística comienza antes del cálculo: comienza con la medición.\nMedir no es simplemente observar un número. Es un proceso mediante el cual se asignan símbolos —generalmente números— a fenómenos del mundo real, siguiendo reglas explícitas definidas por un modelo matemático. Esta asignación conecta dos dominios distintos:\n\nel mundo empírico, donde ocurren los hechos, y\nel mundo abstracto, donde los representamos mediante números.\n\nPor esta razón, toda investigación debe preocuparse cuidadosamente del diseño de la medición, ya que los datos no son un reflejo perfecto de la realidad, sino una representación construida.\n\n  \n  \n    Figura 1: Fenómeno real → Instrumento → Regla de medición → Número.\n  \n\n\n\nInstrumentos de medición: el caso de las encuestas\nUn ejemplo habitual de medición es la encuesta. El cuestionario actúa como instrumento de medición, y sus respuestas generan indicadores numéricos. Sin embargo, que un número exista no garantiza que represente correctamente lo que se desea estudiar.\nPor ello, el diseño del instrumento es crítico. En el caso de encuestas, las preguntas deben:\n\nser claras y comprensibles,\nevitar ambigüedades,\nmedir un solo aspecto a la vez,\nno inducir respuestas,\ny respetar el tiempo del encuestado.\n\nAdemás, al aplicar el instrumento surgen decisiones fundamentales: ¿a cuántas personas medir?, ¿cómo seleccionarlas?, ¿qué hacer ante la no respuesta?, ¿son válidas las respuestas obtenidas?\n\n  \n  \n    Figura 2: pregunta mal formulada vs pregunta bien formulada.\n  \n\n\n\nCaracterísticas fundamentales de una medición\nQué información contienen realmente los números\nNo todos los números entregan el mismo tipo de información. En estadística, las mediciones pueden poseer hasta cuatro propiedades fundamentales, que se incorporan de manera progresiva:\n\nUnicidad: los números identifican categorías distintas.\nOrden: permiten comparar “más” o “menos”.\nDistancia: las diferencias entre valores son significativas y comparables.\nProporción: existe un cero absoluto que permite hablar de razones (doble, mitad).\n\nA medida que se incorporan estas propiedades, aumenta la riqueza informativa del dato y también las posibilidades de análisis estadístico.\n\n\nEscalas de medición\nNominal, ordinal, intervalar y de razón\nLas escalas de medición se definen según las propiedades que cumplen:\n\nEscala nominal: clasifica sin ordenar (etiquetas). Ej.: sexo, región, marca.\nEscala ordinal: ordena sin cuantificar diferencias. Ej.: bajo–medio–alto.\nEscala intervalar: permite comparar diferencias, pero no proporciones. Ej.: temperatura en °C.\nEscala de razón: incorpora un cero absoluto y permite comparaciones proporcionales. Ej.: peso, estatura, ventas.\n\nReconocer la escala de medición es fundamental, ya que determina qué operaciones estadísticas tienen sentido y cuáles no.\n\n  \n  \n    Figura 3:  Cuadro comparativo con ejemplos por tipo de escala.\n  \n\n\n\nError de medición\nMedir siempre implica incertidumbre\nToda medición puede representarse como el resultado de la observación de un fenómeno, denotada por \\(O_i\\). Esta observación se compone de tres elementos fundamentales:\n\n\\(R_i\\): el valor real o verdadero de la medición,\n\\(S_i\\): el error sistemático asociado al proceso de medición,\n\\(A_i\\): el error aleatorio inherente a la observación.\n\nEn una primera aproximación, la relación entre estos componentes puede expresarse mediante el modelo aditivo: \\[\nO_i = R_i + S_i + A_i.\n\\]\nFinalmente, se distinguen dos conceptos clave:\n\nFiabilidad: consistencia de la medición (bajo error aleatorio).\nValidez: grado en que realmente se mide lo que se pretende medir.\n\nUna medición puede ser fiable pero no válida; sin fiabilidad, no puede haber validez.\n\n  \n  \n    Figura 4:  Diana de tiro mostrando combinaciones de precisión y validez.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/intro.html#estadística-y-ciencia",
    "href": "material/modulo_01/intro.html#estadística-y-ciencia",
    "title": "Introducción",
    "section": "Estadística y ciencia",
    "text": "Estadística y ciencia\n\nEl rol de la estadística en la producción de conocimiento científico\nLa expresión método científico no alude a un procedimiento único ni rígido. En la práctica, los científicos utilizan diversos enfoques y herramientas, pero todos comparten un objetivo central: producir conocimiento científico fundamentado.\nEste proceso comienza con preguntas bien formuladas, susceptibles de ser respondidas mediante evidencia empírica y razonamiento riguroso. La investigación científica busca precisamente obtener respuestas confiables a estas interrogantes a través de procedimientos sistemáticos.\nPese a la diversidad de métodos existentes, es posible identificar elementos comunes en toda investigación científica:\n\nrevisión crítica de hechos y teorías previas,\nformulación de hipótesis contrastables,\nevaluación objetiva de resultados y conclusiones.\n\n\n\nEnfoques clásicos y modernos de investigación científica\nTradicionalmente, la investigación científica ha adoptado una postura descriptiva y analítica, en la cual el investigador se considera externo al fenómeno estudiado. Desde esta perspectiva clásica —de raíz cartesiana—, el problema se aborda fragmentando sus componentes y analizando sus relaciones, bajo una estructura conceptual que puede resumirse como:\n\nobservador – problema – entorno\n\nEste enfoque ha sido exitoso en muchos ámbitos, pero presenta limitaciones cuando los fenómenos son complejos, dinámicos o altamente interdependientes.\nEn respuesta a estas limitaciones, han surgido enfoques más recientes inspirados en la Teoría General de Sistemas y la cibernética, que proponen una visión global e integrada del fenómeno. En estos enfoques, el observador forma parte del sistema que estudia, y el conocimiento se construye a partir de una interacción operacional, experimental y perceptual con el fenómeno. La validez del conocimiento depende entonces de la coherencia del proceso de observación dentro de un dominio claramente definido.\n\n  \n  \n    Figura 5:  Enfoque clásico vs Enfoque sistémico\n  \n\n\n\nLa estadística como pilar del método científico\nLa estadística proporciona una forma de pensamiento clara, disciplinada y rigurosa para recolectar, organizar e interpretar información en contextos donde la variabilidad y la incertidumbre son inevitables. En este sentido, no es solo una técnica auxiliar, sino un componente estructural del quehacer científico.\nLa estadística se ocupa de:\n\nrecolectar y clasificar datos,\nresumir información relevante,\nidentificar patrones y regularidades,\nrealizar inferencias bajo incertidumbre, con el objetivo último de apoyar la toma de decisiones y la formulación de predicciones.\n\n\n\nEstadística descriptiva e inferencial\nDesde el punto de vista metodológico, la estadística puede dividirse en dos grandes áreas:\n\nEstadística descriptiva: se centra en describir y resumir un conjunto de datos mediante tablas, gráficos y medidas numéricas, sin pretender generalizar más allá de los datos observados.\nEstadística inferencial: busca extender las conclusiones obtenidas a partir de una muestra hacia una población más amplia, apoyándose en el cálculo de probabilidades para realizar estimaciones, contrastes de hipótesis y predicciones.\n\nEsta distinción es fundamental, ya que marca el paso desde la descripción de los datos hacia la toma de decisiones basada en evidencia.\n\n  \n  \n    Figura 6:  Muestra → inferencia → población",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/intro.html#introducción-al-muestreo",
    "href": "material/modulo_01/intro.html#introducción-al-muestreo",
    "title": "Introducción",
    "section": "Introducción al muestreo",
    "text": "Introducción al muestreo\n\nPor qué inferimos a partir de una parte y no del todo\nEn la mayoría de los problemas reales, la población de interés —personas, objetos, eventos— es demasiado grande como para ser observada en su totalidad. Medir todos sus elementos suele ser costoso, lento o directamente impracticable.\nEl muestreo surge como una solución fundamental: consiste en seleccionar un subconjunto de elementos, llamado muestra, con el objetivo de obtener información confiable sobre la población completa de manera más eficiente.\nLa utilidad del muestreo no radica únicamente en el ahorro de recursos, sino en su capacidad para producir estimaciones válidas, siempre que la muestra sea representativa de la población.\n\n\nRepresentatividad y diseño muestral\nUna muestra es representativa cuando refleja adecuadamente las características relevantes de la población. Cuanto mayor es esta representatividad, mayor es la confiabilidad de las conclusiones que se extraen.\nLas técnicas de muestreo forman un pilar central de la estadística, ya que el alcance de las conclusiones depende directamente de cómo se seleccionaron los datos. No toda muestra permite generalizar resultados.\nDesde el punto de vista metodológico, las observaciones pueden obtenerse:\n\na lo largo del tiempo, o\nmediante un diseño de muestreo.\n\nEl diseño muestral establece los principios que permiten seleccionar muestras representativas al menor costo posible y con la mayor precisión alcanzable.\n\n\nAleatorización: el principio clave del muestreo científico\nEl concepto central del muestreo estadístico es la aleatorización. Seleccionar elementos al azar significa que la inclusión en la muestra no depende de decisiones subjetivas del investigador.\nLa presencia o ausencia de aleatorización permite clasificar los métodos de muestreo en dos grandes categorías:\n\nMuestreo no probabilístico: los resultados describen únicamente a los individuos observados y no permiten inferencias válidas sobre la población.\nMuestreo probabilístico: cada elemento de la población tiene una probabilidad conocida de ser seleccionado, lo que habilita la inferencia estadística.\n\nUna aleatorización deficiente introduce sesgo, incluso si el tamaño muestral es grande.\n\n\nTipos de muestreo probabilístico\nEn el muestreo probabilístico existen diversas técnicas, cuya elección depende del conocimiento previo de la población, del nivel de precisión requerido y de los costos asociados.\n\nMuestreo aleatorio simple (MAS)\nCada elemento de la población tiene la misma probabilidad de ser seleccionado. Es adecuado cuando la población es relativamente homogénea respecto a la característica de interés y se dispone de una lista completa de sus elementos.\nSu lógica es equivalente a una lotería, donde la selección se realiza mediante números aleatorios.\n\n  \n  \n    Figura 7: Puntos distribuidos homogéneamente, algunos seleccionados al azar.\n  \n\n\n\nMuestreo aleatorio sistemático\nLos elementos se seleccionan a intervalos regulares a partir de un arranque aleatorio. Su implementación es simple y eficiente, pero puede introducir sesgos si existe una estructura periódica en el listado de la población.\n\n  \n  \n    Figura 8: Lista numerada con selección cada *k*-ésimo elemento.\n  \n\n\n\nMuestreo aleatorio estratificado\nLa población se divide en estratos homogéneos internamente, pero distintos entre sí. Luego se selecciona una muestra aleatoria dentro de cada estrato.\nEste método:\n\nreduce la variabilidad de los estimadores,\nmejora la precisión,\ny permite estudiar subpoblaciones específicas.\n\n\n  \n  \n    Figura 9: Población coloreada por grupos → muestras dentro de cada grupo.\n  \n\n\n\nMuestreo aleatorio por conglomerados\nLa unidad muestral es un grupo de elementos (conglomerado), no los elementos individuales. Se seleccionan conglomerados completos y, eventualmente, elementos dentro de ellos.\nEs útil cuando la población está organizada en grupos naturales y el acceso a los elementos individuales es costoso.\n\n  \n  \n    Figura 10: Grupos compactos de puntos, algunos seleccionados completos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/univariado.html",
    "href": "material/modulo_01/univariado.html",
    "title": "Organización de Datos Univariados",
    "section": "",
    "text": "Tablas de frecuencia\nLa estadística descriptiva busca sintetizar y organizar conjuntos de datos para identificar patrones y características relevantes del fenómeno estudiado. Mientras una observación individual aporta información limitada, es el análisis conjunto el que permite comprender el comportamiento global de los datos.\nEste proceso se apoya en dos enfoques complementarios: el análisis exploratorio de datos, basado en tablas y gráficos para un diagnóstico inicial, y el uso de medidas descriptivas, que cuantifican propiedades clave como tendencia central y dispersión.\nLa organización de los datos es un paso esencial para facilitar el análisis. Los datos pueden presentarse como no agrupados o agrupados, siendo las tablas de frecuencia una herramienta fundamental, especialmente cuando el volumen de datos es grande o se trabaja con variables continuas.",
    "crumbs": [
      "Estadística descriptiva",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organización de Datos Univariados</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/univariado.html#tablas-de-frecuencia",
    "href": "material/modulo_01/univariado.html#tablas-de-frecuencia",
    "title": "Organización de Datos Univariados",
    "section": "",
    "text": "Resumir datos para facilitar su interpretación\nLas tablas de frecuencia permiten organizar un conjunto de datos asignando a cada categoría o valor el número de veces que aparece en la muestra. Son especialmente útiles en estadística descriptiva, ya que transforman datos dispersos en información interpretable.\nCuando el número de categorías es muy grande, la tabla pierde su capacidad de síntesis y deja de ser un buen resumen.\nSea \\(C_1, C_2, \\dots, C_k\\) el conjunto de clases (categorías) observadas en la muestra.\n\nFrecuencia absoluta \\((n_i)\\): número de observaciones que pertenecen a la clase \\(C_i\\).\n\\[\n\\sum_{i=1}^{k} n_i = n\n\\]\nFrecuencia relativa \\((f_i)\\): proporción de observaciones de la clase \\(C_i\\) respecto del total.\n\\[\nf_i = \\frac{n_i}{n}, \\qquad \\sum_{i=1}^{k} f_i = 1\n\\]\n\nLa frecuencia absoluta indica cuántas veces ocurre una categoría; la frecuencia relativa permite comparar categorías en términos proporcionales.\nEjemplo. Suponga que se desea determinar la frecuencia de las facturas que llegan a una empresa como resultado de trabajos realizados por contratistas de obras menores. Los datos disponibles corresponden a las siguientes empresas:\nEmis Ltda., Baker & Jones, Smith y CIA., Brown e Hijos.\nLas facturas recibidas se registran a continuación:\nEmis Ltda.      Baker & Jones     Smith y CIA.     Emis Ltda.\nEmis Ltda.      Baker & Jones     Baker & Jones    Smith y CIA.\nBrown e Hijos   Emis Ltda.        Smith y CIA.     Baker & Jones\nEmis Ltda.      Baker & Jones     Smith y CIA.     Brown e Hijos\nBaker & Jones   Smith y CIA.      Brown e Hijos    Baker & Jones\nBaker & Jones   Emis Ltda.        Baker & Jones    Emis Ltda.\nSmith y CIA.    Emis Ltda.        Baker & Jones    Emis Ltda.\nBrown e Hijos   Emis Ltda.        Emis Ltda.       Emis Ltda.\nA partir de estos registros se obtiene la siguiente tabla de frecuencias:\nTabla 4.1. Frecuencias de facturas entregadas por empresas contratistas\n\n\n\nEmpresa\nFrecuencia absoluta\nFrecuencia relativa\n\n\n\n\nBaker & Jones\n10\n31,25 %\n\n\nBrown e Hijos\n4\n12,50 %\n\n\nEmis Ltda.\n12\n37,50 %\n\n\nSmith y CIA.\n6\n18,75 %\n\n\nTotal\n32\n100 %\n\n\n\n\n\n\n\n\n\nTipCódigo en R\n\n\n\n\n\n\n# Vector con las empresas que emitieron las facturas\nfacturas &lt;- c(\n  \"Emis Ltda.\", \"Baker & Jones\", \"Smith y CIA.\", \"Emis Ltda.\",\n  \"Emis Ltda.\", \"Baker & Jones\", \"Baker & Jones\", \"Smith y CIA.\",\n  \"Brown e Hijos\", \"Emis Ltda.\", \"Smith y CIA.\", \"Baker & Jones\",\n  \"Emis Ltda.\", \"Baker & Jones\", \"Smith y CIA.\", \"Brown e Hijos\",\n  \"Baker & Jones\", \"Smith y CIA.\", \"Brown e Hijos\", \"Baker & Jones\",\n  \"Baker & Jones\", \"Emis Ltda.\", \"Baker & Jones\", \"Emis Ltda.\",\n  \"Smith y CIA.\", \"Emis Ltda.\", \"Baker & Jones\", \"Emis Ltda.\",\n  \"Brown e Hijos\", \"Emis Ltda.\", \"Emis Ltda.\", \"Emis Ltda.\"\n)\n\n# Frecuencia absoluta\nfreq_abs &lt;- table(facturas)\n\n# Frecuencia relativa\nfreq_rel &lt;- prop.table(freq_abs)\n\n# Tabla final de frecuencias\ntabla_frecuencias &lt;- data.frame(\n  Empresa = names(freq_abs),\n  f = as.vector(freq_abs),\n  fr = round(100 * as.vector(freq_rel), 2)\n)\n\ntabla_frecuencias\n\n        Empresa  f    fr\n1 Baker & Jones 10 31.25\n2 Brown e Hijos  4 12.50\n3    Emis Ltda. 12 37.50\n4  Smith y CIA.  6 18.75\n\n\n\n\n\nLas frecuencias absoluta y relativa permiten resumir datos en cualquier escala de medición.\nCuando los datos están al menos en escala ordinal, es posible definir frecuencias acumuladas, que incorporan la noción de orden.\n\nFrecuencia absoluta acumulada \\((N_i)\\): número total de observaciones que pertenecen a las clases\n\\(C_1, C_2, \\dots, C_i\\).\n\\[\nN_i = \\sum_{j=1}^{i} n_j,\n\\qquad i = 1, 2, \\dots, k\n\\]\nEn particular: \\[\nN_k = \\sum_{j=1}^{k} n_j = n\n\\]\nFrecuencia relativa acumulada \\((F_i)\\): proporción de observaciones que pertenecen a las clases\n\\(C_1, C_2, \\dots, C_i\\) respecto del total de la muestra.\n\\[\nF_i = \\sum_{j=1}^{i} f_j,\n\\qquad i = 1, 2, \\dots, k\n\\]\nEn particular: \\[\nF_k = \\sum_{j=1}^{k} f_j = 1\n\\]\n\nLas frecuencias acumuladas permiten responder preguntas del tipo\n\n“¿cuántos (o qué proporción) de los datos no supera cierta categoría?”\n\nEjemplo: En un conjunto de clientes, se desea determinar la clasificación según el cumplimiento en el pago. Los clientes se clasifican en cuatro categorías: Malos (M), Regulares (R), Buenos (B) y Excelentes (E).\nLos datos observados son los siguientes:\nB R B E E E M B E R\nR M M R R M R B B B\nB B E B B B E B E R\nE M B B E B B B B B\nM R M B B B B E M R\nA partir de estos datos se obtiene la siguiente tabla de frecuencias:\nTabla 4.2. Clasificación de clientes por su cumplimiento en el pago\n\n\n\n\n\n\n\n\n\n\nClasificación\nFrecuencia absoluta\nFrecuencia relativa\nFrecuencia acumulada\nFrecuencia relativa acumulada\n\n\n\n\nMalo\n8\n16 %\n8\n16 %\n\n\nRegular\n9\n18 %\n17\n34 %\n\n\nBueno\n23\n46 %\n40\n80 %\n\n\nExcelente\n10\n20 %\n50\n100 %\n\n\nTotal\n50\n100 %\n50\n100 %\n\n\n\n\n\n\n\n\n\nTipCódigo en R\n\n\n\n\n\n\n# Vector con la clasificación de los clientes\nclientes &lt;- c(\n  \"B\",\"R\",\"B\",\"E\",\"E\",\"E\",\"M\",\"B\",\"E\",\"R\",\n  \"R\",\"M\",\"M\",\"R\",\"R\",\"M\",\"R\",\"B\",\"B\",\"B\",\n  \"B\",\"B\",\"E\",\"B\",\"B\",\"B\",\"E\",\"B\",\"E\",\"R\",\n  \"E\",\"M\",\"B\",\"B\",\"E\",\"B\",\"B\",\"B\",\"B\",\"B\",\n  \"M\",\"R\",\"M\",\"B\",\"B\",\"B\",\"B\",\"E\",\"M\",\"R\"\n)\n\n# Nombres completos de las categorías\nniveles &lt;- c(\"M\" = \"Malo\",\n             \"R\" = \"Regular\",\n             \"B\" = \"Bueno\",\n             \"E\" = \"Excelente\")\n\nclientes &lt;- factor(clientes, levels = names(niveles), labels = niveles)\n\n# Frecuencias absolutas\nfreq_abs &lt;- table(clientes)\n\n# Frecuencias relativas\nfreq_rel &lt;- prop.table(freq_abs)\n\n# Frecuencias acumuladas\nfreq_abs_acum &lt;- cumsum(freq_abs)\nfreq_rel_acum &lt;- cumsum(freq_rel)\n\n# Tabla final\ntabla_4_2 &lt;- data.frame(\n  Clase = names(freq_abs),\n  f = as.vector(freq_abs),\n  fr = round(100 * as.vector(freq_rel), 0),\n  F = as.vector(freq_abs_acum),\n  Fr = round(100 * as.vector(freq_rel_acum), 0)\n)\n\ntabla_4_2\n\n      Clase  f fr  F  Fr\n1      Malo  8 16  8  16\n2   Regular  9 18 17  34\n3     Bueno 23 46 40  80\n4 Excelente 10 20 50 100\n\n\n\n\n\nEjemplo: En un conjunto de clientes, se desea analizar el número de veces que se han atrasado en el pago de su cuenta durante un período determinado.\nLos datos observados son los siguientes:\n0 0 2 4 4 7 0 1 4 0 0 0 0 0 0\n0 0 0 2 0 0 0 4 1 1 0 7 3 8 0\n7 0 3 3 7 1 0 3 0 3 0 0 0 0 1\n2 0 8 0 0 0 4 0 0 3 2 3 3 0 0\nCada valor representa el número de atrasos registrados para un cliente. A partir de estos datos se obtiene la siguiente tabla de frecuencias:\nTabla 4.3. Número de veces que un cliente se ha atrasado en el pago de su cuenta\n\n\n\n\n\n\n\n\n\n\nNúmero de atrasos\nFrecuencia absoluta\nFrecuencia relativa (%)\nFrecuencia acumulada\nFrecuencia acumulada (%)\n\n\n\n\n0\n32\n53.4\n32\n53.4\n\n\n1\n5\n8.3\n37\n61.7\n\n\n2\n4\n6.7\n41\n68.4\n\n\n3\n8\n13.3\n49\n81.7\n\n\n4\n5\n8.3\n54\n90.0\n\n\n5\n0\n0.0\n54\n90.0\n\n\n6\n0\n0.0\n54\n90.0\n\n\n7\n4\n6.7\n58\n96.7\n\n\n8\n2\n3.3\n60\n100.0\n\n\n\nEn variables continuas, la organización de datos es un poco más compleja, se dividen los datos en k grupos o segmentos disjuntos, como se muestra Figura 2. Estos grupos representan las clases y se determina la frecuencia de datos asociado a cada grupo, conformando una tabla de frecuencia agrupada.\n\n  \n  \n    Figura 2:  Segmentación en grupos de datos continuos.\n  \n\nEn este tipo de datos las clases están compuestas por intervalos, luego es necesario buscar un representante de la frecuencia asociada a este intervalo, el cual se conoce como marca de clase. Es común utilizar como marca de clase al valor medio del segmento (intervalo).\n\n\nConstrucción de la Tabla de Frecuencia\nEn la construcción de una tabla de frecuencias, el primer paso consiste en determinar el número de clases o intervalos a utilizar. Una regla ampliamente empleada como primera aproximación es la regla de Sturges.\nRegla de Sturges El número de clases se define como: \\[\nk = 3.3 \\log(n) + 1,\n\\] donde \\(n\\) corresponde al número total de observaciones. Se recomienda utilizar un número impar de clases por razones prácticas que se explicarán más adelante.\nAmplitud de clase\nPara determinar la amplitud \\(a\\) de cada clase, se calcula previamente el rango de los datos: \\[\nR_D = \\max{x_i} - \\min{x_i}.\n\\]\nAdemás, se debe identificar la unidad mínima de conteo \\(u\\) de los datos. Con esto, la amplitud se obtiene mediante: \\[\na = \\frac{R_D + u}{k}.\n\\]\nRango de la tabla\nUna vez determinada la amplitud \\(a\\) (idealmente con un decimal adicional respecto de los datos originales), se define el rango de la tabla como: \\[\nR_T = k \\cdot a.\n\\]\nLa inclusión de \\(u\\) en el cálculo de \\(a\\) no siempre garantiza que \\(R_T &gt; R_D\\), por lo que puede ser necesario realizar un ajuste conveniente en el valor de la amplitud.\nLímites de las clases\nPara construir los límites teóricos de las clases, se comienza con el límite inferior de la primera clase: \\[\nLI_1 = \\min{x_i} - \\frac{D}{2},\n\\] donde \\[\nD = R_T - R_D.\n\\] Si el último dígito de \\(D\\) no es par, se realiza un ajuste apropiado.\nEl límite superior de la primera clase se obtiene sumando la amplitud: \\[\nLS_1 = LI_1 + a.\n\\]\nEste límite se considera abierto para la primera clase y cerrado para la siguiente, de modo que: \\[\nLI_2 = LS_1.\n\\]\nLos límites de las clases restantes se obtienen sumando sucesivamente la amplitud hasta completar las \\(k\\) clases.\n\n  \n  \n    Figura 3: Límites de las clases.\n  \n\nTabla 4.4. Tabla de frecuencia genérica\n\n\n\n\n\n\n\n\n\n\nClases\nFrecuencia absoluta\nFrecuencia relativa\nFrecuencia acumulada\nFrecuencia acumulada relativa\n\n\n\n\n\\([LI_1, LS_1)\\)\n\\(n_1\\)\n\\(f_1\\)\n\\(N_1\\)\n\\(F_1\\)\n\n\n\\([LI_2, LS_2)\\)\n\\(n_2\\)\n\\(f_2\\)\n\\(N_2\\)\n\\(F_2\\)\n\n\n\\([LI_3, LS_3)\\)\n\\(n_3\\)\n\\(f_3\\)\n\\(N_3\\)\n\\(F_3\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\([LI_k, LS_k)\\)\n\\(n_k\\)\n\\(f_k\\)\n\\(N_k\\)\n\\(F_k\\)\n\n\n\nEjemplo: Supóngase que los datos corresponden a los tiempos de espera (en segundos) registrados en una línea telefónica de atención al cliente durante un período de observación.\nLos datos observados son los siguientes:\n47 43 33 52 70 24 55 48 52 52 49 47\n34 48 42 57 65 45 48 63 54 54 46 55\n55 65 36 47 66 51 39 11 44 44 45 44\n53 45 44 43 56 59 56 54 23 23 32 49\n55 49 57 57 55 46 42 52 56 56 42 53\n61 46 53 57 54 49 49 45 36 36 47 52\n25 66 44 54 52 41 54 54 57 57 45 46\n42 54 70 41 49 51 44 52 58 58 44\n55 70 34 68 29 36 52 32 45 45 52\n52 57 41 39 42 37 43 35 38 57 69\nA partir de estos datos se procede a construir una tabla de frecuencias agrupadas, siguiendo los pasos habituales.\nNúmero de clases Utilizando la regla de Sturges: \\[\nk = 1 + 3.322 \\log(n) = 1 + 3.322 \\log(117) \\approx 8.\n\\] Se adopta el número impar más cercano, por lo que se utilizan: \\[\nk = 7 \\text{ clases}.\n\\]\nRango de la muestra \\[\nR_M = \\max{x_i} - \\min{x_i} = 70 - 11 = 59.\n\\]\nAmplitud de clase Considerando una unidad mínima de conteo igual a 1: \\[\na = \\frac{R_M + 1}{k} = \\frac{59 + 1}{7} \\approx 8.6.\n\\]\nRango de la tabla \\[\nR_T = k \\cdot a = 7 \\cdot 8.6 \\approx 60.2.\n\\]\nLa diferencia entre ambos rangos es: \\[\nD = R_T - R_M = 60.2 - 59 \\approx 1.2.\n\\]\nLímites de la primera clase El límite inferior de la primera clase se define como: \\[\nLI_1 = \\min{x_i} - \\frac{D}{2} = 11 - 0.6 = 10.4.\n\\]\nEl límite superior correspondiente es: \\[\nLS_1 = LI_1 + a = 10.4 + 8.6 = 19.0.\n\\]\nLos límites de las clases restantes se obtienen sumando sucesivamente la amplitud.\nTabla 4.5. Tiempo de espera antes de ser atendido\n\n\n\n\n\n\n\n\n\n\n\nTiempos (seg.)\nMarca de clase\nFrecuencia absoluta\nFrecuencia relativa (%)\nFrecuencia acumulada\nFrecuencia acumulada (%)\n\n\n\n\n\\([10.4, 19.0)\\)\n14.7\n1\n0.85\n1\n0.85\n\n\n\\([19.0, 27.6)\\)\n23.3\n4\n3.42\n5\n4.27\n\n\n\\([27.6, 36.2)\\)\n31.9\n11\n9.40\n16\n13.67\n\n\n\\([36.2, 44.8)\\)\n40.5\n22\n18.80\n38\n32.47\n\n\n\\([44.8, 53.4)\\)\n49.1\n39\n33.33\n77\n65.80\n\n\n\\([53.4, 62.0)\\)\n57.7\n30\n25.64\n107\n91.44\n\n\n\\([62.0, 70.6]\\)\n66.3\n10\n8.56\n117\n100.00\n\n\n\n\n\nDiagramas de Tallo y Hoja\nEl uso de tablas de frecuencias agrupadas tiene una desventaja bastante obvia, los datos originales se pierden en el proceso de organización. La Figura 4 muestra algunas situaciones que pueden darse:\n\n  \n  \n    Figura 4:  Representación de la distribución de los datos en rangos de intervalos.\n  \n\nOtra forma de presentar tablas de frecuencias agrupadas es mediante el diagrama de tallo y hoja, el cual permite mostrar los datos de manera clara sin perder la información individual ni la noción de distancia entre observaciones.\nEn este diagrama, el tallo corresponde a la primera parte del número y presenta menor variación, mientras que la hoja está formada por los dígitos restantes. Por ejemplo, el valor 548 puede descomponerse como tallo 5 y hoja 48, o bien como tallo 54 y hoja 8, según el nivel de detalle deseado.\nEl diagrama de tallo y hoja es especialmente útil cuando se trabaja con conjuntos pequeños de datos (generalmente menos de 30 observaciones), ya que en estos casos un histograma aporta poca información adicional.\n 5 | 48   → tallo | hoja\n54 | 8   → tallo | hoja\n**Ejemplo*: Los datos observados corresponden a la proporción de reclamos por pagos incorrectos en las cuentas de consumo mensual de electricidad durante los últimos dos años.\nLos datos son los siguientes:\n7.12  7.89  10.12  8.88  10.02  9.91  9.95  9.90\n10.23 9.12  9.99   12.40 8.65   10.05 10.50 9.87\n8.54  9.72  11.09  11.52 12.30  11.53 16.40 13.24\nLos valores se encuentran en el intervalo comprendido entre 7.12 y 16.40. En esta aplicación se utilizan como tallos los valores enteros desde 7 hasta 16, obteniéndose el siguiente diagrama de tallo y hoja:\nDiagrama de tallo y hoja\n\n\n\nTallo\nHojas\n\n\n\n\n7\n12 89\n\n\n8\n54 65 88\n\n\n9\n12 72 87 90 91 95 99\n\n\n10\n02 05 12 23 50\n\n\n11\n09 52 53\n\n\n12\n03 40\n\n\n13\n24\n\n\n14\n–\n\n\n15\n–\n\n\n16\n40\n\n\n\nOtra forma de representar los datos consiste en destacar su carácter decimal, omitiendo los tallos con frecuencia nula cercanos a los extremos, lo que permite visualizar con mayor claridad la discontinuidad en la distribución:\nDiagrama alternativo\n\n\n\nTallo\nHojas\n\n\n\n\n7\n12 89\n\n\n8\n54 65 88\n\n\n9\n12 72 87 90 91 95 99\n\n\n10\n02 05 12 23 50\n\n\n11\n09 52 53\n\n\n12\n03 40\n\n\n13\n24\n\n\n16\n40\n\n\n\nEn algunos casos, ciertas características de los datos pueden apreciarse mejor si cada tallo se divide en subtallos, dando origen al diagrama de tallo y hoja doble. En este caso, se separa cada tallo en dos partes:\n\na: valores con parte decimal menor a 50\nb: valores con parte decimal mayor o igual a 50\n\nEl diagrama resultante es:\nDiagrama de doble tallo y hoja\n\n\n\nTallo\nHojas\n\n\n\n\n7,a\n12\n\n\n7,b\n89\n\n\n8,a\n–\n\n\n8,b\n54 65 88\n\n\n9,a\n12\n\n\n9,b\n72 87 90 91 95 99\n\n\n10,a\n02 05 12 23\n\n\n10,b\n50\n\n\n11,a\n09\n\n\n11,b\n52 53\n\n\n12,a\n03 40\n\n\n13,a\n24\n\n\n16,a\n40\n\n\n\nUna utilidad adicional de los diagramas de tallo y hoja es que permiten comparar conjuntos de datos, cuando resulta pertinente hacerlo. En este contexto, es posible contrastar los valores correspondientes a ambos años mediante un diagrama comparativo, facilitando el análisis visual de posibles diferencias en la distribución.\n\n\nGráficos\nUn gráfico es otra forma de representar y resumir datos, en el gráfico se pueden se hacer evidentes ciertas características que en una tabla de frecuencias pueden pasar inadvertidas.\nLa representación gráfica de los datos ha logrado un uso creciente en los medios de comunicación y eso se debe en gran parte, a la popularidad y uso de software con amplias representaciones gráficas. Hay disponibilidad de gráficas de muchos tipos, desde aquellas para datos agrupados en tablas de frecuencias hasta datos no agrupados, donde su uso depende en gran medida del tipo de escala empleada. En adelante se ilustran distintos tipos de gráficos comúnmente utilizados.\nGráficos de barras y la gráfica de pastel ( circular ), son los gráficos más comunes y sencillos, usualmente utilizados en datos categóricos. Cuando los datos se presentan en escala nominal, la secuencia en que se presentan las clases es totalmente arbitraria, sin embargo, cuando los datos se presentan en escala ordinal, las clases deben mantener el orden de la escala. A continuación se presentan dos aplicaciones que exponen una serie de gráficos y variaciones de estos.\nEjemplo: La tabla siguiente muestra la proporción de clientes según su sector de ubicación.\nTabla 4.6. Sector de ubicación del cliente\n\n\n\nSector\n1\n2\n3\n4\n5\n6\n\n\n\n\nProporción (%)\n10\n15\n40\n20\n10\n5\n\n\n\n\n  \n  \n    Figura 5:  Gráficas de barra asociada de ubicación del cliente.\n  \n\nA partir de estos datos pueden construirse gráficas de barras, que permiten comparar visualmente la proporción de clientes en cada sector.\nLos gráficos circulares constituyen otra alternativa para representar esta información. El gráfico circular tradicional destaca por su sencillez e interpretación directa, mientras que las versiones en tres dimensiones, aunque visualmente atractivas, pueden distorsionar la percepción relativa de los sectores.\n\n  \n  \n    Figura 6:  Gráficas circulares asociadas al sector del cliente.\n  \n\nEjemplo: Supóngase ahora que se desea analizar el grado de satisfacción de los clientes respecto a los servicios adicionales prestados por la empresa. Para ello, una muestra de 77 clientes clasifica su nivel de satisfacción en las siguientes categorías: Insatisfecho (I), Indiferente (II), Normal (N), Satisfecho con reparos (SR) y Totalmente satisfecho (TS).\nTabla 4.7. Grado de satisfacción por servicios adicionales\n\n\n\nGrado de satisfacción\nFrecuencia\nFrecuencia acumulada\n\n\n\n\nInsatisfecho (I)\n19\n19\n\n\nIndiferente (II)\n21\n40\n\n\nNormal (N)\n33\n73\n\n\nSatisfecho con reparos (SR)\n2\n75\n\n\nTotalmente satisfecho (TS)\n4\n77\n\n\n\nDado que la variable es cualitativa ordinal, existe un orden natural en sus categorías, lo que debe respetarse en su representación gráfica.\n\n  \n  \n    Figura 7:  Gráficas circulares asociadas al sector del cliente.\n  \n\nEn el caso de datos cuantitativos continuos, las representaciones gráficas más habituales son:\n\nel histograma de frecuencias acompañado del polígono de frecuencias, y\nla gráfica de frecuencias acumuladas junto con la ojiva.\n\n\n  \n  \n    Figura 8: Histograma de frecuencia y polígono de frecuencia para los tiempos de espera.\n  \n\n\n  \n  \n    Figura 9: Gráfica de frecuencia acumulada y ojiva para los tiempos de espera antes de su atención.\n  \n\nLa última representación gráfica es utilizada en particular cuando la variable bajo estudio se ha medido en el tiempo (datos longitudinales). Está gráfica, llamada diagrama de dispersión, es de gran utilidad en series de tiempo y control estadístico de la calidad, tiene la particularidad que puede mostrar tendencias de los datos en e tiempo. Consideremos los datos de la aplicación 4.5, pero además agreguemos el tiempo como referencia.\nTabla 4.7. Proporción de reclamos por pagos incorrectos según mes y año\n\n\n\nMes\nAño 1\nAño 2\n\n\n\n\nEnero\n7.12\n8.65\n\n\nFebrero\n7.89\n10.05\n\n\nMarzo\n10.12\n10.50\n\n\nAbril\n8.88\n9.87\n\n\nMayo\n10.02\n8.54\n\n\nJunio\n9.91\n9.72\n\n\nJulio\n9.95\n11.09\n\n\nAgosto\n9.90\n11.52\n\n\nSeptiembre\n10.23\n12.30\n\n\nOctubre\n9.12\n11.53\n\n\nNoviembre\n9.99\n16.40\n\n\nDiciembre\n12.40\n13.24\n\n\n\n\n  \n  \n    Figura 10: Diagrama de Dispersión Porcentaje de reclamos el tiempo.",
    "crumbs": [
      "Estadística descriptiva",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organización de Datos Univariados</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/univariado.html#medidas-de-desempeño",
    "href": "material/modulo_01/univariado.html#medidas-de-desempeño",
    "title": "Organización de Datos Univariados",
    "section": "Medidas de Desempeño",
    "text": "Medidas de Desempeño\nLos indicadores de desempeño han adquirido gran importancia a partir del establecimiento de la filosofía de gestión, calidad total y la aplicación de normas nacionales o internacionales. Son herramientas para la evaluación de la gestión, que proveen valores de referencia con el cual se puedan comparar o proponer metas.\nLas medidas de desempeño son otro medio con el cual se resumen los datos, ya que a través de ellos se establece una medida resumen de alguna particularidad en los datos. Estos indicadores se dividen en tres tipos: medidas de posición, resumen de los datos que representa un lugar definido importante dentro de ellos; medidas de variabilidad o riesgo, que como se podrá apreciar son muy importantes ;y medidas de forma, que tienen una importante relación con un grupo de medidas de posición.\n\nMedidas de Posición\nUna medida de posición es un valor simple que se calcula para un grupo de datos y que se utiliza como una manera de resumir a estos un valor dentro del rango de los datos. Normalmente se desea que el valor sea representativo de todos los valores incluidos en el grupo, estos valores pueden estar relacionados con posiciones de particular interés como los extremos, los cuales se asocian a cuantiles , o valores del centro, llamados de tendencia central.\nLa Media Aritmética : La media aritmética, o promedio, se define como el cociente de la suma de todos los valores entre el número total de valores. En estadística, un “promedio**” es una medida de Tendencia central** para un conjunto de datos.\nEn estadística es normal representar una medida descriptiva de una población, (o parámetro poblacional), mediante letras griegas, en tanto que se utilizan letras romanas para las medidas descriptivas de estadísticas muestrales. Así, la media aritmética para una población de valores se presenta mediante el símbolo μ, en tanto\nque la media aritmética de una muestra se representa mediante el símbolo X. Las expresiones para el cálculo de la media de una población y de una muestra son:\n\\[\n\\mu = \\frac{1}{N}\\sum_{i=1}^{N} X_i\n\\qquad\\text{y}\\qquad\n\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n\\]\nEjemplo : Los pagos de consumo, en una muestra de 15 cuentas en un restaurante, fueron:\n\\[1000, 1000, 2500, 2500, 2500, 3500, 4000,\\] \\[5300, 9000,12500, 13500, 24500, 27500, 30900, 41000\\]\nEl promedio muestral es:\n\\[\n\\bar{X} = \\frac{1}{15}\\sum_{i=1}^{15} X_i = 12.080\n\\]\nCuando se agrupan datos en una distribución de frecuencias, se utiliza el punto medio de cada clase como aproximación de todos los valores contenidos en ella. El punto medio o marca de clase se representa con el símbolo \\(m_i\\) , en donde el subíndice \\(i\\) indica la “clase \\(i\\)”, y se utiliza la letra \\(n_i\\) para representar la frecuencia absoluta observada en la clase respectiva.\nLas fórmulas para la media de la población y de la muestra para datos agrupados son:\n\\[\n\\mu = \\frac{1}{N}\\sum_{i=1}^{k} n_i m_i\n\\qquad\\text{y}\\qquad\n\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{k} n_i m_i\n\\]\nEjemplo : Considerando los datos del tiempo de espera (en segundos) antes de ser atendido se tiene:\n\n\n\n\n\n\n\n\n\nTiempos (seg.)\nMarca de clase\nFrecuencia absoluta\nFrecuencia relativa\n\n\n\n\n\\([10.4,;19.0)\\)\n14.7\n1\n0.85 %\n\n\n\\([19.0,;27.6)\\)\n23.3\n4\n3.42 %\n\n\n\\([27.6,;36.2)\\)\n31.9\n11\n9.40 %\n\n\n\\([36.2,;44.8)\\)\n40.5\n22\n18.80 %\n\n\n\\([44.8,;53.4)\\)\n49.1\n39\n33.33 %\n\n\n\\([53.4,;62.0)\\)\n57.7\n30\n25.64 %\n\n\n\\([62.0,;70.6]\\)\n66.3\n10\n8.56 %\n\n\n\n\\[\n\\bar{X}\n= \\frac{1}{117}\\sum_{i=1}^{k} n_i m_i\n= \\frac{14.7 \\times 1 + 23.3 \\times 4 + \\cdots + 66.3 \\times 10}{117}\n= 48.4 \\; \\text{segundos}\n\\]\nLa gran desventaja de este indicador es su gran sensibilidad a la presencia de datos extremos. Un dato extremo se manifiesta inmediatamente en el promedio, poniendo en duda el ser un valor representativo del centro de los datos.\nLa Mediana : La mediana de un conjunto de datos es el valor que ocupa el lugar central de estos cuando se ordenan en orden de magnitud. Para conjunto de datos, con un número par de elementos, la mediana se calcula como el promedio de los valores centrales.\nEn el caso de estar trabajando con datos dispersos, la expresión para determinar la posición de la mediana en el conjunto (ordenado) es:\n\\[\n\\mathrm{Me} =\n\\begin{cases}\nX_{\\left(\\frac{n+1}{2}\\right)}, & \\text{si } n \\text{ es impar}, \\\\[6pt]\n\\dfrac{1}{2}\\left(X_{\\left(\\frac{n}{2}\\right)} + X_{\\left(\\frac{n}{2}+1\\right)}\\right),\n& \\text{si } n \\text{ es par}.\n\\end{cases}\n\\]\nEn las expresiones anteriores, \\(X\\), representa el valor de dato, mientras que el paréntesis en el subíndice, muestra el lugar que ocupa la mediana dentro del conjunto de datos ordenados.\nEjemplo : Considerando los pagos de consumo, en una muestra de 15 cuentas en un restaurante:\n\\[1000, 1000, 2500, 2500, 2500, 3500, 4000,\\] \\[5300, 9000,12500, 13500, 24500, 27500, 30900, 41000\\]\n\\[\n\\Rightarrow \\mathrm{Me}\n= X_{\\left(\\frac{n+1}{2}\\right)}\n= X_{\\left(\\frac{15+1}{2}\\right)}\n= 5.300\n\\]\nPara datos agrupados, en primer lugar es necesario determinar la clase que contiene el valor de la mediana, para después determinar la posición de la mediana dentro de la clase mediante interpolación. La clase que contiene la mediana es la primera clase cuya frecuencia acumulada es mayor o igual a la mitad de los datos. Una vez que se identifica esta clase, se determina el valor interpolado de la mediana , empleando la siguiente expresión:\n\\[\n\\mathrm{Me}\n= L_i\n+ \\left(\\frac{\\frac{n}{2} - N_{i-1}}{n_i}\\right)a_i\n\\]\n\n\\(L _i\\) = Límite inferior de la clase que contiene la mediana.\n\\(n\\)= número total de observaciones en la distribución de frecuencias.\n\\(a_i\\) Amplitud de clase.\n\\(N_{i-1}\\) = La frecuencia acumulada anterior a la clase que contiene la mediana.\n\\(n_i\\)= Número de observaciones en la clase que contiene la mediana.\n\nLos fundamentos de esta expresión están en la ojiva y la interpolación lineal.\nEjemplo : Para los datos agrupados de la Tabla 4.5, la mediana del tiempo de espera (en segundos) antes de ser atendido es:\n\n\n\n\n\n\n\n\n\nTiempos (seg.)\nMarca de clase\nFrecuencia absoluta\nFrecuencia acumulada\n\n\n\n\n\\([10.4,;19.0)\\)\n14.7\n1\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\([44.8,;53.4)\\)\n49.1\n39\n77\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\nCálculo de la mediana (datos agrupados):\n\\[\n\\mathrm{Me}\n= 44.8\n+ \\left(\\frac{\\frac{117}{2} - 38}{39}\\right)\\,8.6\n= 49.3 \\;\\text{segundos}\n\\]\nLa mediana es otra medida de tendencia central, este indicador no es afectado por datos extremos (indicador robusto).\nLa Moda : Medida de tendencia central, que está dada por el valor o clase que se presenta con mayor frecuencia. A una distribución que tiene una sola moda se le denomina unimodal.\nCuando dos valores no adyacentes tienen frecuencias máximas similares, se dice que la distribución es bimodal.\nPara datos agrupados, primero se identifica la clase que contiene la moda, determinando la clase que tiene el mayor número de observaciones (clase modal). Algunos autores consideran que la moda es el punto medio de la clase modal (marca de clase), otros, interpolan dentro de la clase modal, de acuerdo con la siguiente expresión:\n\\[\n\\mathrm{Mo}\n= L_i\n+ \\left(\\frac{d_1}{d_1 + d_2}\\right)a_i,\n\\qquad\n\\text{donde }\n\\frac{d_1}{d_1 + d_2}\n\\text{ es un factor de ponderación.}\n\\]\n\n\\(L_i\\) = Límite inferior de la clase que contiene la moda.\n\\(d_1\\) = Diferencia entre la frecuencia de la clase modal y la frecuencia de la clase que le precede.\n\\(d_2\\) = Diferencia entre la frecuencia de la clase modal y la frecuencia de la clase que le sigue. \\(a_i\\) = Amplitud del intervalo de clase.\n\nLa deducción de la expresión es clara con la ayuda de la Figura 11. Como se puede apreciar en esta figura \\(d_2\\) es menor que \\(d_1\\) , por esta razón el factor de ponderación es mayor que 0.5, que multiplicado por la amplitud permite que la moda se encuentre más cerca del limite superior de la clase modal.\n\n  \n  \n    Figura 11: Esquema de localización de la moda.\n  \n\nEn caso que \\(d_2\\) sea mayor que \\(d_1\\) , el factor de ponderación es menor que 0.5, que multiplicado a la amplitud permite que la moda se encuentre más cerca del limite inferior. Cuando \\(d_2\\) es igual a \\(d_1\\) , el factor de ponderación es 0.5, que multiplicado a la amplitud permite que la moda se encuentre justo en la marca de clase modal.\nEn tablas de frecuencia es posible encontrar dos o más clases con igual máxima frecuencia, en este caso se dice que la población es: bimodal , trimodal , etc.\nEjemplo : Para los datos agrupados de la Tabla 4.5, la moda de los tiempos de espera (en segundos) es:\n\n\n\n\n\n\n\n\n\nTiempos (seg.)\nMarca de clase\nFrecuencia absoluta\nFrecuencia acumulada\n\n\n\n\n\\([36.2,;44.8)\\)\n40.5\n22\n38\n\n\n\\([44.8,;53.4)\\)\n49.1\n39\n77\n\n\n\\([53.4,;62.0)\\)\n57.7\n30\n107\n\n\n\n\\[\n\\mathrm{Mo}\n= 44.8\n+ \\left(\\frac{39 - 22}{(39 - 22) + (39 - 30)}\\right)\\,8.6\n= 50.4 \\;\\text{segundos}\n\\]\nExisten otras medidas de tendencia central, utilizadas en situaciones más específicas como una solución al problema de la alta sensibilidad del promedio aritmético, alguna de ellas son:\nMedia Geométrica : Se utiliza principalmente para promediar proporciones de variaciones, en datos económicos y se define como la raíz n-ésima del producto de los n valores.\n\\[\nM_G = \\sqrt[n]{x_1 \\cdot x_2 \\cdots x_n}\n\\]\nMedia Armónica : Se define como el recíproco de la media de los recíprocos de las medias, es decir:\n\\[\nM_H = \\frac{n}{\\displaystyle\\sum_{i=1}^{n}\\frac{1}{x_i}}\n\\]\nMedia Recortada : Se define como el valor medio excluyendo un porcentaje de datos en el extremo inferior y superior del conjunto de observaciones. Existen medias recortadas al 90%, 80%, etc. Por ejemplo, en la media recortada al 90%, no se consideran en el cálculo de la media el 5% de los datos más pequeños y el 5% de los datos más grandes.\nCuantiles : Los cuantiles son medidas de posición que dividen los datos en grupos bajo los cuales se encuentra una determinada proporción de éstos, por lo se requiere que los datos se encuentren en al menos escala\nLa mediana es un cuantil que divide la distribución de los datos en dos partes de igual frecuencia acumulada, y luego bajo/sobre la mediana se encuentra acumulado el 50% de los datos. Los cuartiles , la dividen en cuatro cuartos; los quintiles , dividen la población en cinco; los deciles , la dividen en diez décimos; y los puntos percentiles , la dividen en cien partes. Estos, en el caso de datos dispersos, son expresados por:\n\\[\nQ_i \\;(\\text{cuartil } i)\n= X_{\\left(\\frac{i(n+1)}{4}\\right)},\n\\qquad i = 1,2,\\ldots,4\n\\]\n\\[\nK_i \\;(\\text{quintil } i)\n= X_{\\left(\\frac{i(n+1)}{5}\\right)},\n\\qquad i = 1,2,\\ldots,5\n\\]\n\\[\nD_i \\;(\\text{decil } i)\n= X_{\\left(\\frac{i(n+1)}{10}\\right)},\n\\qquad i = 1,2,\\ldots,10\n\\]\n\\[\nP_i \\;(\\text{percentil } i)\n= X_{\\left(\\frac{i(n+1)}{100}\\right)},\n\\qquad i = 1,2,\\ldots,100\n\\]\nEstas expresiones son exactas en la medida que los factores de proporción: \\(\\frac{i(n+1)}{4},\\;\n\\frac{i(n+1)}{10},\\;\n\\frac{i(n+1)}{100}\\) sean números enteros, en caso contrario una buena aproximación (aunque no la única) la entrega el promedio entre el entero superior e inferior de la respectiva fracción, tal como se presenta en la aplicación siguiente.\nEjemplo : Considerando los pagos de consumo:\n\\[1000, 1000, 2500, 2500, 2500, 3500, 4000, 5300,\\] \\[9000,12500, 13500, 24500, 27500, 30900,41000\\]\n\\[\n\\Rightarrow Q_3\n= X_{\\left(\\frac{3(15+1)}{4}\\right)}\n= X_{(12)}\n= 24.500\n\\]\nLuego, el 75% de los pagos por consumo son menores o iguales a $ 24.500.\nAlgunos casos en que el factor de proporción no resulta un número entero, como por ejemplo, el decil 4 ó el percentil 68. En el primero, el valor se encuentra entre los valores sexto y séptimo del grupo ordenado, cuya interpretación sería que el 40% de los importes de consumo de las 15 cuentas del restaurante son menores o iguales a $ 3.750.\n\\[\nD_4\n= X_{\\left(\\frac{4(15+1)}{10}\\right)}\n= X_{(6.4)}\n= \\frac{X_{(6)} + X_{(7)}}{2}\n= 3.750\n\\]\nEn el segundo cuantil, el valor se encuentra entre los valores 10 y 11 del grupo ordenado.\n\\[\nP_{68}\n= X_{\\left(\\frac{68(15+1)}{100}\\right)}\n= X_{(10.88)}\n= \\frac{X_{(10)} + X_{(11)}}{2}\n= 13.000\n\\]\nPara datos agrupados, la fórmula se modifica de acuerdo con el punto fraccionario de interés. Para utilizar esta expresión modificada, en primer lugar se determina la clase que contiene el punto de interés, de acuerdo con las frecuencias acumuladas, y después se lleva a cabo una interpolación como en el caso anterior de la mediana. Ahora un análisis más exhaustivo de estas expresiones se obtiene a través del segmento de la línea recta en la ojiva, recordemos la Figura 4 de los tiempos de espera, donde a partir de 117 datos, se construye la gráfica, de la Figura 4.10 y supongamos que estamos interesados en el percentil 78, por lo tanto debemos determinar, de acuerdo con las frecuencias acumuladas la clase que contiene el punto de interés , como se muestra en la Figura 12, este punto se encuentra en la penúltima clase.\n\n  \n  \n    Figura 12: Gráfica para la determinación de percentiles.\n  \n\nSi se recuerda la ecuación de la línea de la recta, dada por:\n\\[\n\\frac{y - y_1}{y_2 - y_1}\n=\n\\frac{x - x_1}{x_2 - x_1}\n\\]\nEn este caso se observa que:\n\\[\ny = \\textbf{y} \\qquad\nx = P_{78} \\qquad\nx_2 - x_1 = LS - LI = a \\qquad\ny_2 - y_1 = N_i - N_{i-1} = n_i\n\\]\nLuego despejando \\(x = P_{78}\\) , se obtiene una expresión para el cálculo de percentiles en datos agrupados:\n\\[\nx = P_{78}\n= x_1 + \\left(\\frac{y - y_1}{y_2 - y_1}\\right)(x_2 - x_1)\n= LI + \\left(\\frac{y - N_{i-1}}{n_i}\\right)a\n\\]\nEn el futuro se debe notar que \\(y\\) no es otra cosa que \\(\\dfrac{n \\times j}{100}\\), donde \\(j\\) es el percentil j-ésimo.\nLuego, la expresión general para el cálculo de percentiles, utilizando frecuencias absolutas como relativas está dada por:\n\\[\nP_j\n= LI\n+ \\left(\\frac{\\frac{n \\cdot j}{100} - N_{i-1}}{n_i}\\right)a\n= LI\n+ \\left(\\frac{\\frac{j}{100} - F_{i-1}}{f_i}\\right)a\n\\]\nEsta expresión claramente permite el cálculo de deciles, quintiles o cuartiles.\nEjemplo : Para los datos agrupados en Tabla 4.5, el percentil 80 de los tiempos de espera (en segundos) es:\n\n\n\n\n\n\n\n\n\nTiempos (seg.)\nMarca de clase\nFrecuencia absoluta\nFrecuencia acumulada\n\n\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\([44.8,;53.4)\\)\n49.1\n39\n77\n\n\n\\([53.4,;62.0)\\)\n57.7\n30\n107\n\n\n\\([62.0,;70.6)\\)\n66.3\n10\n117\n\n\n\n\\[\nP_{80}\n= 53.4\n+ \\left(\\frac{\\frac{117 \\cdot 80}{100} - 77}{30}\\right)\\,8.6\n= 58.2 \\;\\text{segundos}\n\\]\nLuego, el 80% de los tiempos de espera es menor o igual a los 58,2 segundos.\nOtra utilidad, de la expresión anterior, permite determinar que porcentaje de los datos se encuentra bajo (o por defecto sobre) un determinado valor, como por ejemplo, ¿Qué porcentaje de las veces, los tiempos de espera fueron superiores a 47 segundos?. En este caso se conoce el percentil, pero no el porcentaje, luego:\n\\[\n47\n= 44.8\n+ \\left(\\frac{\\frac{117 \\cdot j}{100} - 38}{39}\\right)\\,8.6\n\\;\\;\\Longrightarrow\\;\\;\nj = 41.01\\%\n\\]\nPor lo tanto, el (100 – 40,01)%= 58,99%. son superiores a 47 segundos.\nTambién se puede determinar el porcentaje de tiempos de servicio que se encuentra en el intervalo [47; 63] segundos. Como se sabe el porcentaje que se encuentra bajo los 47 segundos (40,01%),y determinando el % que están bajo los 63 segundos.\n\\[\n63\n= 62.0\n+ \\left(\\frac{\\frac{117 \\cdot j}{100} - 107}{10}\\right)\\,8.6\n\\;\\;\\Longrightarrow\\;\\;\nj = 92.45\\%\n\\]\nObteniéndose que el porcentaje de tiempos de servicio en el intervalo deseado es de (92,45 – 40,01)% = 51,44%.\n\n\nMedidas de Variabilidad\nLas medidas de tendencia central ó de posición que se presentaron son útiles para identificar un valor “típico” ó “particular” de un conjunto de datos, las medidas de variabilidad se ocupan de describir la dispersión (riesgo, precisión) de los datos con respecto a una medida del centro o un valor particular.\nA modo de ejemplo, suponga que dos máquinas empacadoras dan como resultado productos con un peso promedio de 10 gramos, pero que en un caso los productos se encuentran dentro de un rango de 0,1 gramos con respecto a este peso promedio, en tanto que en el otro los pesos pueden variar hasta en un gramo. Como se observa en la Figura 13, en el primer caso los datos son menos dispersos respecto al valor de 10 gramos que en el segundo caso, lo que implicaría que suposiciones realizadas al primer caso serían de menor riesgo que las del segundo.\n\n  \n  \n    Figura 13: Visualización de la variabilidad en un conjunto de datos.\n  \n\nExisten varios indicadores para medir la magnitud de la variabilidad en conjuntos de datos. Las que se describen a continuación son: rango , rango modificado , desviación media , varianza, desviación estándar y coeficiente de variación.\nEl rango: El rango \\((R)\\) es la diferencia entre el mayor y el menor valor del conjunto de datos. Si \\(\\max{x_i}\\) representa el valor máximo y \\(\\min{x_i}\\) el valor mínimo, entonces:\n\\[\nR =\n\\begin{cases}\n\\max\\{x_i\\} - \\min\\{x_i\\}, & \\text{datos no agrupados},\\\\[6pt]\nLS_k - LI_1, & \\text{datos agrupados}.\n\\end{cases}\n\\]\nEjemplo: Considerando los pagos de consumo en una muestra de 15 cuentas en un restaurante:\n\\[1000, 1000, 2500, 2500, 2500, 3500, 4000,\\] \\[5300, 9000,12500, 13500, 24500, 27500, 30900, 41000\\]\nel rango es:\n\\[\nR = \\max\\{x_i\\} - \\min\\{x_i\\}\n= 41000 - 1000\n= 40{,}000.\n\\]\nEjemplo: Para los datos agrupados de la Tabla 4.5, el rango de los tiempos de espera (en segundos) es:\n\n\n\n\n\n\n\n\n\nTiempos (seg.)\nMarca de clase\nFrecuencia absoluta\nFrecuencia acumulada\n\n\n\n\n\\([10.4,;19.0)\\)\n14.7\n1\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\([62.0,;70.6)\\)\n66.3\n10\n117\n\n\n\n\\[\nR = LS_7 - LI_1\n= 70.6 - 10.4\n= 60.2 \\;\\text{segundos}.\n\\]\nRangos Modificados: Un rango modificado es un rango para el cual se elimina cierto porcentaje de los valores en cada uno de los extremos de la distribución y se simboliza por \\(R_{\\text{mod}}) ((j%) \\text{ central})\\). Algunos rangos modificados típicos son el 50% central, el 80% central y el 90% central.\nPara determinar el rango modificado, primero se deben ubicar los dos percentiles de interés y, posteriormente, calcular el rango entre ellos. Por ejemplo, para el rango del 80% central, los percentiles de interés son el décimo percentil y el nonagésimo percentil, ya que el 80% central de los valores se encuentra entre estos dos puntos.\nEjemplo: Considerando los pagos de consumo en una muestra de 15 cuentas en un restaurante:\n\\[1000, 1000, 2500, 2500, 2500, 3500, 4000,\\] \\[5300, 9000,12500, 13500, 24500, 27500, 30900, 41000\\]\nel rango modificado al 50% central está dado por:\n\\[\nP_{75} = X_{\\left(\\frac{75(n+1)}{100}\\right)} = X_{(12)} = 24{,}500.\n\\]\n\\[\nP_{25} = X_{\\left(\\frac{25(n+1)}{100}\\right)} = X_{(4)} = 2{,}500.\n\\]\n\\[\nR_{\\text{Mod}}(50\\% \\text{ central}) = P_{75} - P_{25}\n= 24{,}500 - 2{,}500 = 22{,}000.\n\\]\nEl rango modificado al 50% central también es conocido como rango intercuartílico, mientras que el rango modificado al 80% es conocido como rango interdecílico. Los rangos modificados, en general, buscan anular el efecto de valores extremos de los datos, que producirían un fuerte efecto en el rango tradicional, como medida de variabilidad.\nEjemplo Para los datos agrupados de la Tabla 4.5, el rango modificado al 90% central de los tiempos de espera (en segundos) es:\n\\[\nP_{5} = 27{,}6\n+ \\left(\\frac{\\frac{117 \\cdot 5}{100} - 5}{11}\\right)\\,8{,}6\n= 28{,}3\\;[\\text{seg.}]\n\\]\n\\[\nP_{95} = 62{,}0\n+ \\left(\\frac{\\frac{117 \\cdot 95}{100} - 107}{10}\\right)\\,8{,}6\n= 65{,}6\\;[\\text{seg.}]\n\\]\n\\[\nR_{\\text{Mod}}(90\\% \\text{ central})\n= P_{95} - P_{5}\n= 65{,}6 - 28{,}3\n= 37{,}3\\;[\\text{segundos}].\n\\]\nLa Desviacion Media : La desviación media (DM) es la media (promedio) del valor absoluto de la diferencia entre cada uno de los datos y el promedio del grupo.\n\\[\n\\mathrm{DM} =\n\\begin{cases}\n\\displaystyle \\sum_{i=1}^{n} \\frac{\\lvert x_i - \\bar{x} \\rvert}{n},\n& \\text{datos dispersos}, \\\\[10pt]\n\\displaystyle \\sum_{i=1}^{k} f_i \\lvert m_i - \\bar{x} \\rvert,\n& \\text{datos agrupados}.\n\\end{cases}\n\\]\n\nNota : Algunos autores utilizan la diferencia entre cada valor y la mediana.\n\nEs común también utilizar indicadores como la desviación mediana (DMe) o la desviación modal (DMo), como indicadores de variabilidad alternativos a la desviación media. La utilización de estos indicadores, es debido a la alta sensibilidad del promedio a valores extremos, que también se hereda en indicadores que utilizan este indicador, como es el caso de la desviación media. También debe notarse, que la desviación modal, sólo es posible cuando la moda se determina a partir de datos cuantitativos, ya que es una medida de dispersión que no tiene sentido en datos cualitativos donde no existe la noción de distancia.\nEjemplo : Considere que las ventas (por vendedor) de aparatos eléctricos fueron las siguientes: \\(5 – 8 – 8 11 – 11 – 11 – 14 – 16\\). La media aritmética es 10,5. y la desviación media es:\n\n\n\n\\(x_i\\)\n\\(x_i - \\bar{x}\\)\n\\(\\lvert x_i - \\bar{x} \\rvert\\)\n\n\n\n\n5\n-5,5\n5,5\n\n\n8\n-2,5\n2,5\n\n\n8\n-2,5\n2,5\n\n\n11\n0,5\n0,5\n\n\n11\n0,5\n0,5\n\n\n11\n0,5\n0,5\n\n\n14\n3,5\n3,5\n\n\n16\n3,5\n3,5\n\n\nTotal\n\n21,0\n\n\n\n\\[\n\\Rightarrow \\mathrm{DM} = \\frac{1}{8}\\sum_{i=1}^{8} \\lvert x_i - \\bar{x} \\rvert\n= \\frac{21}{8}\n= 2{,}625 \\approx 2{,}6 \\text{ unidades}\n\\]\nAsí, puede decirse que, en promedio, las ventas de aparatos eléctricos por vendedor difieren en 2,6 unidades de la media del grupo,\nEjemplo : Para los datos agrupados de la Tabla 4.5, para los tiempos de espera, la media aritmética es 48,4 segundos, la desviación media está dada por:\n\n\n\n\n\n\n\n\n\n\n\\(f_i\\)\n\\(m_i\\)\n\\(m_i - \\bar{x}\\)\n\\(\\lvert m_i - \\bar{x} \\rvert\\)\n\\(f_i \\lvert m_i - \\bar{x} \\rvert\\)\n\n\n\n\n0,009\n14,7\n-33,7\n33,7\n0,3033\n\n\n0,034\n23,3\n-25,1\n25,1\n0,8534\n\n\n0,094\n31,9\n-16,5\n16,5\n1,5510\n\n\n0,188\n40,5\n-7,9\n7,9\n1,4852\n\n\n0,333\n49,1\n0,7\n0,7\n0,2331\n\n\n0,256\n57,7\n9,3\n9,3\n2,3808\n\n\n0,086\n66,3\n17,9\n17,9\n1,5394\n\n\nTotal\n\n\n\n8,3462\n\n\n\n\\[\n\\Rightarrow \\mathrm{DM} = \\sum_{i=1}^{k} f_i \\lvert m_i - \\bar{x} \\rvert\n\\approx 8{,}4 \\text{ segundos}\n\\]\nLa Varianza y la Desviación Estándar : La varianza es similar a la desviación media porque se basa en la diferencia entre cada uno de los valores del conjunto de datos y la media del grupo, La diferencia consiste en que, antes de sumarlas, se eleva al cuadrado cada una de las diferencias, Para una población, se representa la varianza mediante \\(V(X)\\) o, típicamente por la letra \\(\\sigma^2\\) ; la fórmula de cálculo es:\n\\[\nV(X) = \\sigma^2 = \\sum_{i=1}^{N} \\frac{(x_i - \\mu)^2}{N}\n\\]\nA diferencia de otras estadísticas muestrales que se han analizado, la varianza de una muestra no es, en términos de cálculo, completamente equivalente a la varianza de la población, La varianza muestral se representa mediante \\(S^2\\) , y está dada por:\n\\[\nS^2 = \\sum_{i=1}^{n} \\frac{(x_i - \\bar{x})^2}{,n-1,}\n\\]\nSe utiliza con mayor frecuencia la raíz cuadrada de la varianza, representada mediante la letra griega σ para el caso poblacional y \\(S\\) para una muestra, y se le denominada desviación estándar, Las fórmulas son:\n\\[\n\\sigma = \\sqrt{V(X)}\n\\]\nEstas medidas (muestrales) también tienen su representación en datos agrupados, la cual está dada por:\n\\[\nS = \\sqrt{\\text{Varianza muestral}}\n\\]\nLa desviación estándar , además de ser una medida de dispersión que utiliza toda la información (en contraposición con los rangos) y ser expresada en igual unidad de medida que los datos originales, es especialmente útil cuando se le utiliza junto con la denominada distribución normal.\nEjemplo : Para los datos de ventas de aparatos eléctricos: \\(5 – 8 – 8\n11 – 11 – 11 – 14 – 16\\). , la media aritmética es 10,5 unidades. Considerando estos datos mensuales de ventas como la población estadística de interés, se determina la desviación estándar:\n\\[\n\\sigma = \\sqrt{V(X)} = \\sqrt{\\frac{86}{8}} = \\sqrt{10{,}75} \\approx 3{,}3 \\text{ unidades}\n\\]\nEjemplo : Para los datos agrupados de la Tabla 4.5 (tiempos de espera), la media aritmética es 48,4 segundos, la desviación estándar es:\n\n\n\n\n\n\n\n\n\n\n\\(f_i\\)\n\\(m_i\\)\n\\(m_i - \\bar{x}\\)\n\\((m_i - \\bar{x})^2\\)\n\\(f_i (m_i - \\bar{x})^2\\)\n\n\n\n\n0,009\n14,7\n-33,7\n1135,7\n10,2212\n\n\n0,034\n23,3\n-25,1\n630,0\n21,4203\n\n\n0,094\n31,9\n-16,5\n272,3\n25,5915\n\n\n0,188\n40,5\n-7,9\n62,4\n11,7331\n\n\n0,333\n49,1\n0,7\n0,5\n0,1632\n\n\n0,256\n57,7\n9,3\n86,5\n22,1414\n\n\n0,086\n66,3\n17,9\n320,4\n27,5533\n\n\nTotal\n\n\n\n118,826\n\n\n\n\\[\n\\Rightarrow S = \\sqrt{V(X)} = \\sqrt{118{,}826} \\approx 10{,}9\n\\]\n\n\nMedidas de Forma\nLos indicadores de forma de las distribuciones de frecuencias asociadas a un conjunto de datos, son medidas que se agrupan en : asimetría y curtosis.\nLas medidas de asimetría centran su interés en la tendencia de los datos a concentrarse en los valores más pequeños, que se conoce como asimetría positiva; valores más grandes, que conoce como asimetría negativa; o simplemente en el centro, que se denomina como simétrica.\nEn la Figura 14, se observan las posibilidades de asimetría, en conjuntos de datos que presentan sólo una cima. En estos casos observar tendencia de los datos resulta fácil, sin embargo cuando se presenta más de una cima en una gráfica (no necesariamente más de una moda), determinar la asimetría de los datos es más riesgoso, con lo cual se recomienda el uso de indicadores para una mayor certeza de la situación. La figura muestra además, el caso de datos simétricos de un conjunto de datos bimodales.\nLas medidas de curtosis centran su atención en la tendencia de los datos en el grado de concentración que estos poseen alrededor de puntos centrales, en este caso se dice que los datos tienen una concentración mesocúrtica cuando el grado de concentración se acerca a ‘ lo ideal ’, mientras que, se habla de letocúrtica o platicúrtica , si el grado de concentración es menor o mayor a lo ideal,respectivamente.\nEl concepto de ideal asociado a esta medida, tiene su fundamento en la comparación de la curtosis muestral (mediante el uso de indicadores), con el de un modelo de probabilidad muy importante en estadística clásica que posee un valor teórico fijo con el cual es comparado.\n\n  \n  \n    Figura 14: Representación de simetría en conjunto de datos.\n  \n\nEn la Figura 14, se muestran las tres situaciones de curtosis, en conjuntos de datos que presentan tan sólo una cima. Se puede apreciar, que en el caso de distribuciones leptocúrticas, la menor variabilidad es evidente en comparación a las otras formas. La distribución mesocúrtica, representa el caso de una distribución con variabilidad ideal , en comparación con el modelo probabilístico Normal. Finalmente la distribución platicúrtica, representa la mayor variabilidad en comparación con la distribución ideal.\n\n  \n  \n    Figura 15: Representaciones de curtosis en conjunto de datos.\n  \n\nSin embargo, mediante un análisis gráfico es muy difícil poder visualizar que un conjunto de datos posee alguno de estos patrones.,\nLos indicadores asociados a curtosis, ayudan a la comparación de la variabilidad en los datos, pues justamente la variabilidad mide el grado de no concentración de estos. Adicionalmente, se pueden emplear como un criterio para determinar la existencia de datos extremos, es decir, muy grandes ó muy pequeños, con respecto al común de los datos observados, que causarían un efecto devastador en algunos indicadores, ó bien, como una señal de la existencia de dos estratos dentro de los datos que se analizan, como en el caso de la Figura 15, donde se muestra un conjunto de datos bimodales, donde perfectamente, se podría suponer que en la característica de la población en estudio se presenta concentrada en dos grupos, que afectarían los resultados de algunos indicadores.\nA continuación se presentan una serie de indicadores asociados a características de forma, en el primer caso se muestran indicadores de asimetría dados por los coeficientes de: Yule, Simetría, Pearson y Fisher; para finalizar con les coeficientes de curtosis: \\(K_2\\) y Fisher.\nCoeficiente de Yule y Simetría : Estos son dos indicadores de simetría, que se basan en cuantiles centrales, como lo son: cuartil 1, cuartil 3 y la mediana. Las expresiones de cálculo de Yule y Simetría son:\n\\[\nI_Y = \\frac{Q_3 + Q_1 - 2Q_2}{2Q_2}\n\\]\n\\[\nI_S = \\frac{Q_3 + Q_1 - 2Q_2}{Q_3 - Q_1}\n\\]\nEstos indicadores (adimensionales) son de fácil cálculo, tanto para datos dispersos como agrupados, tienen la ventaja de no ser afectados por observaciones aberrantes, que siempre se encuentran sobre \\(Q_3\\) o bajo \\(Q_1\\) , razón por la cuál se puede\napreciar que ambos indicadores muestran la simetría en el centro de los datos y no en la totalidad de éstos.\nCoeficiente de Pearson : El coeficiente de Pearson, se basa en tres indicadores de usual uso en estadística y mide la asimetría, como la diferencia entre la media y la mediana con respecto a la desviación estándar. Este coeficiente poblacional y muestral se encuentran dados respectivamente por:\n\\[\nA_S = \\frac{3(\\bar{x} - M_e)}{S_x}\n\\]\n\\[\nA_S = \\frac{3(\\mu - M_e)}{\\sigma}\n\\]\nEl promedio y la mediana, que son dos medidas de tendencia central, que cuando hay simetría siempre son iguales, y la desviación estándar, que es una medida de riesgo que estandariza el indicador, hacen de éste, un indicador más completo.\nCoeficiente de Simetría de Fisher : Es el indicador de simetría más fiable de los presentados anteriormente, se basa en el tercer momento de la distribución de los datos, y que para datos dispersos y agrupados se obtiene mediante:\n\\[\nm_3 = \\sum_{i=1}^{n} \\frac{(x_i - \\bar{x})^3}{n}\n\\]\n\\[\nm_3 = \\sum_{i=1}^{k} f_i (m_i - \\bar{x})^3\n\\]\nEstas medidas se ven fuertemente afectadas por las unidades de medida de los datos en estudio, por lo tanto se estandariza para medir la asimetría estandarizada, cuya expresión queda:\n\\[\n\\alpha_3 = \\frac{m_3}{S_x^3}\n\\]\nEn su cálculo poblacional, al igual que en el coeficiente de Pearson, basta con el reemplazo de los indicadores muestrales: \\(\\overline{x}\\) y \\(s\\) , por sus respectivos cálculos poblaciones \\(\\mu\\) y \\(\\sigma\\).\nEl punto de comparación teórico de estos indicadores es el cero , pues en distribuciones simétricas todos los indicadores resultan ser cero, mientras que si el indicador en negativo o positivo, se dice que la asimetría es negativa o positiva, respectivamente. Sin embargo, en la práctica en el análisis de datos reales, nunca se obtienen coeficientes ‘cero’, por lo cual es bueno recomendar un intervalo en torno al cual se aceptará la simetría.\nPara una distribución simétrica el valor del coeficiente de asimetría es cero, porque el promedio y la mediana son iguales, mientras que para una distribución con asimetría positiva la media es siempre mayor que la mediana y, por ello el valor del coeficiente es positivo, como se muestra en la Figura 15, donde además se muestra el caso de que en una distribución con un coeficiente de asimetría negativa, la media es siempre menor que la mediana.\n\n  \n  \n    Figura 15:  distribución de las medidas de tendencia central en curvas unimodales.\n  \n\nEjemplo : Para los datos de ventas de aparatos eléctricos: \\(5 – 8 – 8 – 11 – 11\n11 – 14 – 16\\). La media aritmética, la mediana, el primer y tercer cuartil, además de la desviación estándar están dadas por: 10.5; 11.0; 8.0; 12.5 y 3.3 unidades, respectivamente.\nConsiderando que estos datos mensuales de ventas son la población estadística de interés, se tiene que:\n\\[\nI_Y = \\frac{Q_3 + Q_1 - 2Q_2}{2Q_2} = -0{,}07\n\\]\n\\[\nI_S = \\frac{Q_3 + Q_1 - 2Q_2}{Q_3 - Q_1} = -0{,}33\n\\]\n\\[\nA_S = \\frac{3(10{,}5 - 11{,}0)}{3{,}3} = -0{,}45\n\\]\n\\[\nm_3 = 1{,}5\n\\]\nLuego, si consideramos \\(I_Y\\), \\(I_S\\) y \\(A_S\\) , la distribución tiene una ligera asimetría\nnegativa es decir, “esta sesgada hacia la izquierda”, sin embargo si usamos \\(m_3\\)\nmuestra el caso contrario. Esto se debe que tanto \\(I_Y\\), \\(I_S\\) y \\(A_S\\) , a perdido información al\nresumir los datos, por esta razón el \\(m_3\\) un coeficiente más confiable en establecer el\ntipo de asimetría de los datos.\nEjemplo : Para los datos agrupados de la Tabla 4.5 (tiempos de espera), la media aritmética es 48,4 segundos, se obtienen los siguientes resultados:\n\n\n\n\n\n\n\n\n\n\n\\(f_i\\)\n\\(m_i\\)\n\\(m_i - \\bar{x}\\)\n\\((m_i - \\bar{x})^3\\)\n\\(f_i (m_i - \\bar{x})^3\\)\n\n\n\n\n0,009\n14,7\n-33,7\n-38272,75\n-344,45\n\n\n0,034\n23,3\n-25,1\n-15813,25\n-537,65\n\n\n0,094\n31,9\n-16,5\n-4492,13\n-422,26\n\n\n0,188\n40,5\n-7,9\n-493,04\n-92,69\n\n\n0,333\n49,1\n0,7\n0,34\n0,11\n\n\n0,256\n57,7\n9,3\n804,36\n205,92\n\n\n0,086\n66,3\n17,9\n5735,34\n493,24\n\n\nTotal\n\n\n\n-697,79\n\n\n\nUtilizando las medidas calculadas anteriormente como:\n\n\\(\\bar{x} = 48{,}4 \\text{ segundos}\\)\n\\(M_e = 49{,}3 \\text{ segundos}\\)\n\\(S^2 = 118{,}3 \\; (\\text{segundos})^2\\) \\(\\Rightarrow S = 10{,}9 \\text{ segundos}\\)\n\\(Q_1 = 41{,}4 \\text{ segundos}\\)\n\\(Q_3 = 56{,}5 \\text{ segundos}\\)\n\nPor lo tanto se tiene:\n\\(I_Y = -0{,}01\\), \\(I_S = -0{,}05\\), \\(A_S = -0{,}25\\), \\(m_3 = -697{,}79\\), \\(\\alpha_3 = -0{,}54\\)\nCoeficiente \\(K_2\\) : Este indicador de curtosis, que se basan en cuantiles extremos, como lo son: decil 1 y decil 9. Las expresiones de cálculo están dadas por:\n\\[\nK_2 = \\frac{D_9 - D_1}{1{,}9\\, (Q_3 - Q_1)} - 1\n\\]\nEstos indicador de fácil cálculo, tanto para datos dispersos como agrupados, tienen la ventaja de no ser afectados por observaciones aberrantes, que en la mayor parte de los casos se siempre se encuentran sobre el \\(D_ 9\\) o bajo el \\(D_ 1\\). \\(K_2\\) , se encuentra\ndivido por el factor 1,9 veces el rango intercuartílico que es la distancia que existe teóricamente entre los deciles 9 y 1, en la curva ideal estandarizada.\nCoeficiente de Curtosis de Fisher : Es el indicador de curtosis más, que se base en el cuarto momento de la distribución de los datos, que se encuentra dado para datos dispersos y agrupados por:\n\\[\nm_4 = \\sum_{i=1}^{n} \\frac{(x_i - \\bar{x})^4}{n}\n\\]\n\\[\nm_4 = \\sum_{i=1}^{k} f_i (m_i - \\bar{x})^4\n\\]\nAl igual que en el caso de m 3 , esta medida se ven fuertemente influenciadas por las unidades de medida de los datos en estudio, por lo tanto, esta medida de desempeño para medir la curtosis se estandariza, cuya expresión queda de la siguiente manera: \\[\n\\alpha_4 = \\frac{m_4}{S_x^{\\,4}} - 3\n\\]\nEn su cálculo poblacional, basta con el reemplazo de los indicadores muestrales: \\(\\overline{x}\\) y \\(s\\) , por sus respectivos cálculos poblaciones \\(\\mu\\) y \\(\\sigma\\).\nEl punto de comparación teórico de estos indicadores es el cero, pues en distribuciones absolutamente mesocurticas todos los indicadores resultan ser cero, mientras que si el indicador en negativo o positivo, se dice que la curtosis es platicurtica o leptocurtica, respectivamente.\nSin embargo, en análisis de datos continuos en la práctica, nunca se obtienen coeficientes ‘cero’, por lo cual es bueno recomendar un intervalo en torno al cual se acepta la distribución de los datos como mesocurtica.\nEjemplo : Para los datos de ventas de aparatos eléctricos que fueron: \\(5 – 8\n8 – 11 – 11 – 11 – 14 – 16\\). Donde el primer y noveno decil, junto con el primer y tercer cuartil están dados por: 5; 16; 8 y 12.5 unidades, respectivamente.\nConsiderando que estos datos mensuales de ventas son la población estadística de interés, se determina los coeficientes de curtosis:\n\\[\nK_2 = \\frac{16 - 5}{1{,}9 \\times (12{,}5 - 8)} - 1 = 0{,}29\n\\]\n\\[\nm_4 = 257{,}31\n\\qquad \\Rightarrow \\qquad\n\\alpha_4 = -0{,}83\n\\]\nLuego, si consideramos \\(K_2\\) , la distribución tiene un agudamiento que se podría considerar mesocúrtico, sin embargo si usamos \\(m_4\\) muestra una clara tendencia platicúrtica. Esto se debe que \\(K_2\\) , a perdido información al resumir los datos, por esta razón el \\(m_4\\) un coeficiente más confiable en establecer el tipo de curtosis de los datos.\nEjemplo : Para los datos agrupados de la Tabla 4.5, para los tiempos de espera, donde la media aritmética es 48,4 segundos, la desviación estándar está dada por:\n\n\n\n\n\n\n\n\n\n\n\\(f_i\\)\n\\(m_i\\)\n\\(m_i - \\bar{x}\\)\n\\((m_i - \\bar{x})^4\\)\n\\(f_i (m_i - \\bar{x})^4\\)\n\n\n\n\n0,009\n14,7\n-33,7\n1289791,78\n11608,13\n\n\n0,034\n23,3\n-25,1\n396912,60\n13495,03\n\n\n0,094\n31,9\n-16,5\n74120,06\n6967,29\n\n\n0,188\n40,5\n-7,9\n3895,01\n732,26\n\n\n0,333\n49,1\n0,7\n0,24\n0,08\n\n\n0,256\n57,7\n9,3\n7480,52\n1915,01\n\n\n0,086\n66,3\n17,9\n102662,57\n8828,98\n\n\nTotal\n\n\n\n43546,78\n\n\n\nUtilizando las medidas calculadas anteriormente como:\n\n\\(Q_1 = 41{,}4 \\text{ segundos}\\)\n\\(Q_3 = 56{,}5 \\text{ segundos}\\)\n\\(D_1 = 32{,}8 \\text{ segundos}\\)\n\\(D_9 = 70{,}1 \\text{ segundos}\\)\n\nPor lo tanto se tiene:\n\\[\nK_2 = 0{,}30\n\\]\n\\[\nm_4 = 43546{,}78\n\\]\n\\[\n\\alpha_4 = 0{,}08\n\\]",
    "crumbs": [
      "Estadística descriptiva",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organización de Datos Univariados</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/multivariado.html",
    "href": "material/modulo_01/multivariado.html",
    "title": "Organización de Datos Multivariados",
    "section": "",
    "text": "Aplicación 5.1\nEn el caso de la estadística multivariada, ya no se mide una sola característica por cada elemento o unidad poblacional (o muestral), sino un conjunto de \\(p\\) características simultáneamente. Esta situación puede representarse mediante una matriz de datos, como se ilustra en la Figura 5.1, donde cada fila corresponde a una unidad observacional y cada columna a una variable medida.\nSugerencia de imagen:\nMatriz de datos multivariados (filas = individuos, columnas = variables).\nEn este contexto, el análisis exploratorio de datos resulta fundamental para observar patrones de comportamiento en el conjunto de variables. El análisis gráfico suele ser de gran ayuda, ya que permite identificar grupos de elementos cuyo comportamiento es similar en varias características. Sin embargo, cuando el número de observaciones es grande, el uso indiscriminado de gráficos puede generar una cantidad excesiva de visualizaciones, dificultando la interpretación global.\nUna forma de abordar este problema consiste en trabajar con muestras representativas del conjunto de datos. La recomendación general es utilizar dos o tres muestras que, si presentan rasgos similares, permitan extender las conclusiones a la población completa.\nEntre las gráficas más utilizadas en este tipo de análisis se encuentra la matriz de asociaciones, que muestra la relación entre pares de variables cuantitativas. Cuando las variables no son cuantitativas, existen alternativas gráficas, tales como:\nConsidérese una muestra de clientes a los cuales se les han medido las siguientes características:\nLos datos observados se presentan a continuación:\nEn la Figura 5.2 se muestra una matriz de gráficas de asociación, donde en la diagonal aparecen histogramas de frecuencia de cada variable y fuera de ella, gráficos de dispersión entre pares de variables.\nSugerencia de imagen:\nMatriz de dispersión con histogramas en la diagonal.\nEn dichas gráficas se observan claramente dos conjuntos de observaciones, lo que sugiere la posible existencia de dos grupos de clientes con características similares dentro de cada grupo y diferentes entre grupos.\nEsta apreciación se ve reforzada mediante las caras de Chernoff, mostradas en la Figura 5.3, donde cada variable se representa mediante un rasgo facial distinto. A grandes rasgos, se identifican dos tipos de caras, lo que apoya la hipótesis de agrupación.\nSugerencia de imagen:\nCaras de Chernoff representando cinco variables.\nSin embargo, las gráficas de perfiles (Figura 5.4) introducen cierto grado de duda, ya que parecen sugerir la existencia de más de dos grupos. Por ejemplo:\nEstos antecedentes conducen a la creencia a priori de que podrían existir hasta cuatro grupos dentro de las observaciones.\nSugerencia de imagen:\nGráficas de perfiles por caso.",
    "crumbs": [
      "Estadística descriptiva",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Organización de Datos Multivariados</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/multivariado.html#organización-de-datos",
    "href": "material/modulo_01/multivariado.html#organización-de-datos",
    "title": "Organización de Datos Multivariados",
    "section": "Organización de Datos",
    "text": "Organización de Datos\nA medida que aumenta el número de características medidas, se vuelve prácticamente imposible organizar los datos de manera efectiva sin perder claridad. Para extraer información relevante, una estrategia común es utilizar tablas de doble entrada, donde se seleccionan pares de características de interés.\n\nTablas de Frecuencia de Doble Entrada\nSea \\(n_{ij}\\) la cantidad de unidades que pertenecen a la clase \\(i\\)-ésima de la característica \\(B\\) y a la clase \\(j\\)-ésima de la característica \\(A\\).\n\n\n\nCaracterística B  Característica A\nA1\nA2\n…\nAr\n\n\n\n\nB1\n\\(n_{11}\\)\n\\(n_{12}\\)\n…\n\\(n_{1r}\\)\n\n\nB2\n\\(n_{21}\\)\n\\(n_{22}\\)\n…\n\\(n_{2r}\\)\n\n\n…\n…\n…\n…\n…\n\n\nBk\n\\(n_{k1}\\)\n\\(n_{k2}\\)\n…\n\\(n_{kr}\\)\n\n\n\nEstas cantidades corresponden a frecuencias absolutas. La tabla también puede expresarse en términos de frecuencias relativas, definidas como:\n\\[\nf_{ij} = \\frac{n_{ij}}{n}\n\\]\ndonde \\(n\\) es el tamaño total de la muestra.\n\n\n\nAplicación 5.2\nSe realiza una encuesta a profesionales de una región, midiendo las variables:\n\nCargo\n\nSueldo\n\nValor del automóvil (en miles de pesos)\n\nAl tratar la variable ingreso, que es continua, se requiere agruparla en clases. En la siguiente tabla se presentan las frecuencias absolutas para las variables ingreso y cargo:\n\n\n\nIngresos [$]\nOtros\nVentas\nAdm.\nEjecut.\nSub-G.\nGerente\n\n\n\n\n[119–514[\n8\n7\n8\n30\n0\n0\n\n\n[514–909[\n14\n7\n21\n26\n0\n0\n\n\n[909–1.303[\n4\n3\n8\n16\n4\n0\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\nDado que las clases tienen distinta amplitud, el histograma bivariado debe ajustarse a dichas amplitudes.\nSugerencia de imagen:\nHistograma bivariado Ingreso vs. Cargo.\nLa tabla siguiente presenta las frecuencias relativas conjuntas:\n\n\n\nIngresos [$]\nOtros\nVentas\nAdm.\nEjecut.\nSub-G.\nGerente\n\n\n\n\n[119–514[\n0,041\n0,036\n0,041\n0,155\n0,000\n0,000\n\n\n[514–909[\n0,072\n0,036\n0,109\n0,134\n0,000\n0,000\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\n\n\n\nFrecuencias Marginales\nLas frecuencias marginales se obtienen sumando por filas o columnas:\n\\[\nn_{i\\bullet} = \\sum_{j=1}^{r} n_{ij}, \\qquad\nn_{\\bullet j} = \\sum_{i=1}^{k} n_{ij}\n\\]\nEn términos relativos:\n\\[\nf_{i\\bullet} = \\frac{n_{i\\bullet}}{n}, \\qquad\nf_{\\bullet j} = \\frac{n_{\\bullet j}}{n}\n\\]\n\n\n\nAplicación 5.3\nA partir de los datos anteriores, se observa que la frecuencia marginal del ingreso se ve fuertemente afectada por la gran amplitud del último intervalo, lo que refuerza la necesidad de ajustar por amplitud al interpretar resultados.\n\n\n\nAplicación 5.4\nSe analizan las variables horas de trabajo semanal y desempeño laboral (escala 1–9):\n\n\n\nDesempeño\n15–25\n25–35\n35–45\n\n\n\n\n1–3\n16\n18\n12\n\n\n4–6\n11\n14\n9\n\n\n7–9\n9\n7\n4\n\n\n\nLas frecuencias relativas conjuntas y marginales evidencian una marcada asimetría en la variable desempeño.",
    "crumbs": [
      "Estadística descriptiva",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Organización de Datos Multivariados</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/multivariado.html#frecuencias-condicionales",
    "href": "material/modulo_01/multivariado.html#frecuencias-condicionales",
    "title": "Organización de Datos Multivariados",
    "section": "Frecuencias Condicionales",
    "text": "Frecuencias Condicionales\nLas frecuencias condicionales permiten analizar una variable dado un valor específico de otra. Se definen como:\n\\[\nf_{i|j} = \\frac{n_{ij}}{n_{\\bullet j}}, \\qquad\nf_{j|i} = \\frac{n_{ij}}{n_{i\\bullet}}\n\\]\n\n\nAplicación 5.5\nSe presentan las frecuencias relativas del ingreso condicionadas al cargo, lo que permite calcular indicadores como el promedio condicional por cargo.\nSugerencia de imagen:\nGráfica de promedios de ingreso por cargo.",
    "crumbs": [
      "Estadística descriptiva",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Organización de Datos Multivariados</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/multivariado.html#asociación-de-variables",
    "href": "material/modulo_01/multivariado.html#asociación-de-variables",
    "title": "Organización de Datos Multivariados",
    "section": "5.3 Asociación de Variables",
    "text": "5.3 Asociación de Variables\nEn estadística descriptiva multivariada, cada unidad observacional se representa mediante un vector de \\(p\\) características. Para pares de variables al menos ordinales, una herramienta fundamental es el diagrama de dispersión, que representa los pares:\n\\[\n(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\n\\]\nSugerencia de imagen:\nGráfico de dispersión clásico.\n\nCoeficiente de Correlación de Pearson\nEl grado de asociación lineal entre dos variables cuantitativas se mide mediante el coeficiente de correlación de Pearson:\n\\[\nr = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}\n{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n\\]\nEste coeficiente toma valores en el intervalo ([-1, 1]$. Valores cercanos a \\(\\pm 1\\) indican una fuerte asociación lineal, mientras que valores cercanos a 0 indican ausencia de asociación lineal.\nSugerencia de imagen:\nConjunto de diagramas de dispersión mostrando distintos valores de $r $.\n\n\n\nAplicación 5.6\nSe estudia la relación entre la demanda de artefactos, su precio, el ingreso medio y el precio de un bien sustituto. A partir del análisis marginal y de los diagramas de dispersión, se calculan los coeficientes de correlación de Pearson:\n\n\n\n\\(r_{P}\\)\nPrecio Artefacto\nIngreso Medio\nPrecio Sustituto\n\n\n\n\nPrecio Artefacto\n1,0000\n-0,9096\n-0,8830\n\n\nIngreso Medio\n-0,9096\n1,0000\n0,8681\n\n\nPrecio Sustituto\n-0,8830\n0,8681\n1,0000\n\n\n\nEstos resultados confirman una fuerte asociación lineal, positiva o negativa según el par de variables analizado, coherente con las observaciones gráficas previas.\n\n\nCorrelación para datos agrupados\nCuando las variables se presentan en forma agrupada en tablas de doble entrada, el coeficiente de correlación de Pearson puede calcularse utilizando las marcas de clase de cada intervalo.\nSea \\(m_{x_i}\\) la marca de clase de la \\(i\\)-ésima clase de la variable $X $, y \\(m_{y_j}\\) la marca de clase de la \\(j\\)-ésima clase de la variable $Y $. El coeficiente de correlación de Pearson para datos agrupados se define como:\n\\[\nr_P =\n\\frac{\n\\sum_{i=1}^{k} \\sum_{j=1}^{r}\nf_{ij}\n\\left(m_{x_i} - \\bar{x}\\right)\n\\left(m_{y_j} - \\bar{y}\\right)\n}{\n\\sqrt{\n\\sum_{i=1}^{k}\nf_{i\\bullet}\n\\left(m_{x_i} - \\bar{x}\\right)^2\n}\n\\sqrt{\n\\sum_{j=1}^{r}\nf_{\\bullet j}\n\\left(m_{y_j} - \\bar{y}\\right)^2\n}\n}\n\\]\ndonde:\n\n\\(f_{ij}\\) corresponde a la frecuencia relativa conjunta\n\n\\(f_{i\\bullet}\\) y \\(f_{\\bullet j}\\) son las frecuencias relativas marginales\n\n\\(\\bar{x}\\) y \\(\\bar{y}\\) representan las medias aproximadas de las variables\n\nEste coeficiente conserva las mismas propiedades interpretativas que el coeficiente de Pearson para datos no agrupados.\n\n\n\nAplicación 5.7\nA partir de la tabla de doble entrada que relaciona ingreso mensual y valor del automóvil, se calculan los coeficientes de correlación correspondientes, utilizando como aproximación las marcas de clase de cada intervalo.\nLos resultados obtenidos muestran una asociación positiva moderada entre ambas variables, coherente con la interpretación económica del fenómeno: a mayor ingreso, mayor es el valor promedio del automóvil adquirido.\nSugerencia de imagen:\nDiagrama de dispersión construido a partir de marcas de clase ponderadas por frecuencia.",
    "crumbs": [
      "Estadística descriptiva",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Organización de Datos Multivariados</span>"
    ]
  },
  {
    "objectID": "material/modulo_01/multivariado.html#medidas-de-asociación-para-variables-cualitativas",
    "href": "material/modulo_01/multivariado.html#medidas-de-asociación-para-variables-cualitativas",
    "title": "Organización de Datos Multivariados",
    "section": "Medidas de Asociación para Variables Cualitativas",
    "text": "Medidas de Asociación para Variables Cualitativas\nCuando las variables son cualitativas u ordinales, el coeficiente de correlación de Pearson deja de ser apropiado. En estos casos, se emplean medidas de asociación basadas en tablas de contingencia.\n\n\nCoeficiente Chi-cuadrado de Independencia\nEl estadístico chi-cuadrado permite evaluar si existe independencia entre dos variables cualitativas.\nSea \\(O_{ij}\\) la frecuencia observada en la celda \\((i,j)\\) y \\(E_{ij}\\) la frecuencia esperada bajo el supuesto de independencia:\n\\[\nE_{ij} = \\frac{n_{i\\bullet} n_{\\bullet j}}{n}\n\\]\nEl estadístico se define como:\n\\[\n\\chi^2 =\n\\sum_{i=1}^{k} \\sum_{j=1}^{r}\n\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\nValores altos del estadístico indican evidencia contra la hipótesis de independencia.\nSugerencia de imagen:\nTabla de contingencia con frecuencias observadas y esperadas resaltadas.\n\n\n\nCoeficiente de Contingencia de Pearson\nA partir del estadístico chi-cuadrado se define el coeficiente de contingencia:\n\\[\nC = \\sqrt{\\frac{\\chi^2}{\\chi^2 + n}}\n\\]\nEste coeficiente toma valores en el intervalo $[0,1) $, donde valores cercanos a 0 indican baja asociación y valores cercanos a 1 indican alta asociación.\n\n\n\nCoeficiente V de Cramér\nUna medida alternativa y ampliamente utilizada es el coeficiente V de Cramér, definido como:\n\\[\nV = \\sqrt{\n\\frac{\\chi^2}{n \\cdot \\min(k-1, r-1)}\n}\n\\]\nEste coeficiente presenta la ventaja de estar normalizado en el intervalo ([0,1]$, lo que facilita la comparación entre tablas de distinto tamaño.\n\n\n\nAplicación 5.8\nSe estudia la relación entre el nivel educacional y la categoría ocupacional de un grupo de individuos. El cálculo del estadístico chi-cuadrado y del coeficiente V de Cramér muestra una asociación significativa entre ambas variables, lo que sugiere que el nivel educacional influye en la categoría ocupacional alcanzada.\nSugerencia de imagen:\nGráfico de barras apiladas representando la distribución condicional de ocupación según nivel educacional.",
    "crumbs": [
      "Estadística descriptiva",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Organización de Datos Multivariados</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html",
    "href": "material/modulo_02/probabilidad.html",
    "title": "Elementos de Probabilidad",
    "section": "",
    "text": "Introducción\nEn la investigación científica, por lo general, se requiere de modelos que ayuden a comprender el fenómeno bajo estudio. En un amplio conjunto de situaciones no es posible contar con modelos exactos, también conocidos como modelos determinísticos. En tales casos, las mediciones obtenidas presentan perturbaciones no controlables, lo que provoca variabilidad en los resultados aun cuando los experimentos se realicen bajo condiciones supuestamente idénticas. Esta variabilidad introduce una componente de azar o aleatoriedad en los resultados, dificultando la posibilidad de predecirlos con certeza.\nPor ejemplo, al determinar la resistencia a la ruptura de una barra de acero con una especificación determinada, es esperable que, al medir diez barras distintas, ninguna presente exactamente el mismo valor. Surge entonces la pregunta: ¿qué valor de resistencia debería ofrecerse como especificación del producto?, ¿la resistencia de la barra 1, 2, 3, …, 10?, ¿la mínima?, ¿la máxima? Una respuesta habitual podría ser la resistencia media, aunque este indicador no siempre resulta el más adecuado.\nEn los campos de investigación donde no es posible utilizar modelos determinísticos, es natural aceptar que la predicción no será exacta. Sin embargo, la presencia de fenómenos aleatorios o estocásticos no implica ausencia total de regularidad. Por el contrario, estas mediciones suelen exhibir ciertos patrones, cuya identificación conduce al concepto de ley de probabilidad.\nEl objetivo inicial de este módulo es repasar el concepto de probabilidad desde distintos enfoques.",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#introducción",
    "href": "material/modulo_02/probabilidad.html#introducción",
    "title": "Elementos de Probabilidad",
    "section": "",
    "text": "Enfoque clásico\nEl enfoque clásico, también denominado apriorista, se basa en la asignación de probabilidades a partir de los antecedentes teóricos de un experimento realizado de manera metódica, en el cual todos los resultados posibles son igualmente probables. Este tipo de experimentos se denomina equiprobable y es característico de los juegos de azar.\nPor ejemplo, en un juego de cartas, todas las cartas de un mazo bien barajado tienen la misma probabilidad de ser extraídas. Si se define el evento “extraer una carta roja”, la probabilidad está dada por el cociente entre el número de resultados favorables y el número total de resultados posibles.\nFórmula sugerida:\n\\[\nP(A) = \\frac{\\#\\text{ resultados favorables}}{\\#\\text{ resultados posibles}} = \\frac{\\#R}{\\#S}\n\\]\nEn este enfoque, las probabilidades se determinan antes de realizar el experimento, razón por la cual se denomina enfoque a priori.\nAPLICACIÓN 6.1\nEn un mazo de 52 cartas bien barajadas que contiene 4 ases, la probabilidad de obtener un as en una extracción es:\n\\[\nP(\\text{As}) = \\frac{4}{52} = \\frac{1}{13}\n\\]\n\n\n\nEnfoque frecuentista\nEn el enfoque frecuentista o de frecuencia relativa, la probabilidad se determina a partir de la proporción de veces que ocurre un resultado favorable en un número elevado de observaciones o experimentos. No se asume equiprobabilidad entre los resultados.\nDado que este enfoque se basa en la observación y recopilación de datos, también se denomina enfoque empírico. Las probabilidades no se asignan a priori, sino que se estiman a partir de la experiencia.\nFórmula sugerida:\n\\[\nP(A) \\approx \\frac{n_i}{n}\n\\]\ndonde $ n_i $ es el número de veces que ocurre el evento y $ n $ el número total de observaciones.\nAPLICACIÓN 6.2\nEn una muestra de 10.000 adultos, 100 presentaron un problema dental específico durante el año anterior. La probabilidad estimada es:\n\\[\nP(\\text{Problema dental}) = \\frac{100}{10\\,000} = 0{,}01 = 1\\%\n\\]\n\n\n\nEnfoque bayesiano\nTanto el enfoque clásico como el frecuentista producen probabilidades objetivas, en el sentido de que describen tasas de ocurrencia a largo plazo. En el enfoque bayesiano, en cambio, la probabilidad se interpreta como un grado de creencia o confianza subjetiva respecto a la ocurrencia de un evento.\nDebido a que esta probabilidad depende del juicio personal y de la información disponible, se habla de un enfoque subjetivo, ampliamente utilizado en el análisis bayesiano de decisiones.\nAPLICACIÓN 6.3\nUn inversionista decide comprar terrenos solo si la probabilidad de obtener una plusvalía de al menos 50% en cuatro años es mayor o igual a 0,90. Tras analizar información económica y de mercado, estima dicha probabilidad en 0,75. Dado que este valor es inferior al umbral requerido, la inversión no se realiza.\n\n\n\nDesarrollo axiomático de la probabilidad\nLa probabilidad se fundamenta formalmente en la Teoría de la Medida, para lo cual se introducen las siguientes definiciones.\nDefinición 6.1 (Espacio muestral).\nEl espacio muestral, denotado por $ $, es el conjunto de todos los posibles resultados de un experimento aleatorio.\nDefinición 6.2 (Evento).\nUn evento es cualquier subconjunto del espacio muestral $ $.\nEl conjunto potencia $ 2^{} $ corresponde al conjunto de todos los subconjuntos de $ $. Un subconjunto $ ^{} $ se denomina sigma-álgebra si cumple:\n\n$ $\nSi $ A $, entonces $ A^c $\nSi $ {A_n}{n=1}^{} \\(, entonces\\)$ {n=1}^{} A_n $$\n\nEl par $ (, ) $ se denomina espacio medible. Una función $ P: $ es una medida de probabilidad si satisface:\n\n$ 0 P(A) $, para todo $ A $\n$ P() = 1 $\nSi $ A_1, A_2, $ son disjuntos: \\[\nP\\left(\\bigcup_{i=1}^{n} A_i\\right) = \\sum_{i=1}^{n} P(A_i)\n\\]\n\n\n\n\nTécnicas de conteo\n\nPrincipio de multiplicación\nSi un procedimiento 1 puede realizarse de $ n_1 $ formas y un procedimiento 2 de $ n_2 $ formas, y ambos pueden realizarse secuencialmente, entonces el procedimiento conjunto puede realizarse de:\n\\[\nn_1 \\times n_2\n\\]\nSugerencia de imagen (Figura 6.1):\nDiagrama en árbol ilustrando el principio multiplicativo.\nAPLICACIÓN 6.4\nEn un proceso de manufactura con 4 controles que admiten 3, 4, 2 y 2 resultados posibles, respectivamente, el número total de combinaciones es:\n\\[\n3 \\times 4 \\times 2 \\times 2 = 48\n\\]\n\n\n\nPrincipio de adición\nSi un procedimiento 1 puede realizarse de $ n_1 $ formas y un procedimiento 2 de $ n_2 $ formas, y ambos no pueden ocurrir simultáneamente, entonces el número total de formas es:\n\\[\nn_1 + n_2\n\\]\nSugerencia de imagen (Figura 6.2):\nEsquema ilustrativo del principio aditivo.\nAPLICACIÓN 6.5\nSi existen 3 universidades tradicionales, 5 privadas y 4 centros técnicos, el número total de opciones es:\n\\[\n3 + 5 + 4 = 12\n\\]\n\n\n\n\nFactorial\nDefinición 6.3 (Factorial).\nPara $ n $:\n\\[\nn! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 1\n\\]\n\n\n\nPermutaciones\nCuando el orden importa, se utiliza el concepto de permutación.\nDefinición 6.4 (Permutación).\nEl número de permutaciones de $ r $ elementos tomados de $ n $ es:\n\\[\nP(n,r) = \\frac{n!}{(n-r)!}\n\\]\nAPLICACIÓN 6.6\nPara elegir presidente, secretario y tesorero de entre 10 candidatos:\n\\[\nP(10,3) = \\frac{10!}{7!} = 720\n\\]\n\n\n\nCombinaciones\nCuando el orden no importa, se utiliza la combinatoria.\nDefinición 6.5 (Combinatoria).\n\\[\n\\binom{n}{r} = \\frac{n!}{r!(n-r)!}\n\\]\nAPLICACIÓN 6.7\nPara formar un comité de 3 personas de un grupo de 10:\n\\[\n\\binom{10}{3} = 120\n\\]",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#cálculo-de-probabilidades",
    "href": "material/modulo_02/probabilidad.html#cálculo-de-probabilidades",
    "title": "Elementos de Probabilidad",
    "section": "Cálculo de Probabilidades",
    "text": "Cálculo de Probabilidades\nEn el enfoque clásico, la probabilidad de un evento se define como el cociente entre los casos favorables y los posibles.\nPropiedades básicas:\n\\[\n0 \\le P(A) \\le 1\n\\]\n\\[\nP(A) + P(A^c) = 1\n\\]\n\n\nEventos mutuamente excluyentes\nDos eventos son mutuamente excluyentes si no pueden ocurrir simultáneamente.\nRegla de adición (eventos excluyentes):\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n\nEventos no mutuamente excluyentes\nRegla de adición general:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#probabilidad-condicional-y-eventos-independientes",
    "href": "material/modulo_02/probabilidad.html#probabilidad-condicional-y-eventos-independientes",
    "title": "Elementos de Probabilidad",
    "section": "Probabilidad Condicional y Eventos Independientes",
    "text": "Probabilidad Condicional y Eventos Independientes\nLa probabilidad condicional se define como:\n\\[\nP(B \\mid A) = \\frac{P(A \\cap B)}{P(A)}\n\\]\nDos eventos son independientes si:\n\\[\nP(B \\mid A) = P(B)\n\\]\n\n\nRegla multiplicativa\nPara dos eventos:\n\\[\nP(A \\cap B) = P(A) P(B \\mid A)\n\\]\nPara tres eventos:\n\\[\nP(A \\cap B \\cap C) = P(A) P(B \\mid A) P(C \\mid A \\cap B)\n\\]",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#regla-de-bayes",
    "href": "material/modulo_02/probabilidad.html#regla-de-bayes",
    "title": "Elementos de Probabilidad",
    "section": "Regla de Bayes",
    "text": "Regla de Bayes\nLa regla de Bayes permite actualizar probabilidades a priori:\n\\[\nP(A \\mid B) = \\frac{P(A) P(B \\mid A)}{P(A) P(B \\mid A) + P(A^c) P(B \\mid A^c)}\n\\]",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#variables-aleatorias",
    "href": "material/modulo_02/probabilidad.html#variables-aleatorias",
    "title": "Elementos de Probabilidad",
    "section": "Variables Aleatorias",
    "text": "Variables Aleatorias\nUna variable aleatoria es una función que asigna valores reales a los resultados de un experimento aleatorio.\nDefinición 6.6.\nUna variable aleatoria $ X $ es una función medible:\n\\[\nX: \\Omega \\to \\mathbb{R}\n\\]\n\n\nVariables aleatorias discretas\nDefinición 6.7.\nUna variable aleatoria es discreta si su recorrido es numerable.\n\n\n\nVariables aleatorias continuas\nDefinición 6.8.\nUna variable aleatoria es continua si su recorrido es no numerable.\n\n\n\nFunción de distribución acumulada\nDefinición 6.9.\n\\[\nF_X(x) = P(X \\le x)\n\\]\n\n\n\nFunción de masa de probabilidad\nDefinición 6.10.\n\\[\nf_X(x) = P(X = x)\n\\]\ny cumple:\n\\[\nf_X(x) = F_X(x) - F_X(x-1)\n\\]\n\n\n\nFunción de densidad de probabilidad\nDefinición 6.11.\nUna función $ f_X(x) $ es densidad de probabilidad si:\n\\[\nf_X(x) \\ge 0, \\quad \\int_{-\\infty}^{\\infty} f_X(x)\\,dx = 1\n\\]\nSugerencia de imagen:\nCurva continua representando una función de densidad con área total igual a 1.",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#valor-esperado",
    "href": "material/modulo_02/probabilidad.html#valor-esperado",
    "title": "Elementos de Probabilidad",
    "section": "Valor Esperado",
    "text": "Valor Esperado\nEl valor esperado representa el promedio teórico de una variable aleatoria y constituye uno de los conceptos fundamentales de la teoría de probabilidad.\nmarkdown ### Valor esperado de una variable aleatoria discreta\nSea $ X $ una variable aleatoria discreta que toma los valores $ x_1, x_2, , x_n $ con probabilidades asociadas $ p_1, p_2, , p_n $, respectivamente. El valor esperado de $ X $, denotado por $ (X) $, se define como:\n\\[\n\\mathbb{E}(X) = \\sum_{i=1}^{n} x_i \\, p_i\n\\]\nEste valor puede interpretarse como el promedio ponderado de los valores posibles de la variable, donde las ponderaciones corresponden a las probabilidades de ocurrencia.\nAPLICACIÓN 6.8\nConsidérese una variable aleatoria $ X $ que representa el número de defectos en un producto, con la siguiente distribución:\n\n\n\n\\(x_i\\)\n0\n1\n2\n3\n\n\n\n\n\\(p_i\\)\n0,40\n0,35\n0,20\n0,05\n\n\n\nEl valor esperado es:\n\\[\n\\mathbb{E}(X) = 0(0{,}40) + 1(0{,}35) + 2(0{,}20) + 3(0{,}05) = 0{,}95\n\\]\n\n\nValor esperado de una variable aleatoria continua\nSea $ X $ una variable aleatoria continua con función de densidad de probabilidad $ f_X(x) $. El valor esperado se define como:\n\\[\n\\mathbb{E}(X) = \\int_{-\\infty}^{\\infty} x f_X(x)\\,dx\n\\]\nEste valor representa el centro de gravedad de la distribución.\nSugerencia de imagen:\nCurva de densidad continua con una línea vertical indicando el valor esperado.\n\n\n\nPropiedades del valor esperado\nSean $ X $ y $ Y $ variables aleatorias y $ a, b $. El valor esperado cumple las siguientes propiedades:\n\nLinealidad\n\\[\n\\mathbb{E}(aX + b) = a \\mathbb{E}(X) + b\n\\]\nSuma de variables\n\\[\n\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\n\\]\n\nEstas propiedades se mantienen aun cuando las variables no sean independientes.",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#varianza-y-desviación-estándar",
    "href": "material/modulo_02/probabilidad.html#varianza-y-desviación-estándar",
    "title": "Elementos de Probabilidad",
    "section": "Varianza y Desviación Estándar",
    "text": "Varianza y Desviación Estándar\nLa varianza mide la dispersión de los valores de una variable aleatoria respecto de su valor esperado.\n\nVarianza de una variable aleatoria discreta\nSea $ X $ una variable aleatoria discreta con valor esperado $ = (X) $. La varianza de $ X $, denotada por $ (X) $, se define como:\n\\[\n\\operatorname{Var}(X) = \\mathbb{E}\\big[(X - \\mu)^2\\big]\n\\]\nUna forma equivalente y computacionalmente más conveniente es:\n\\[\n\\operatorname{Var}(X) = \\mathbb{E}(X^2) - \\mu^2\n\\]\ndonde:\n\\[\n\\mathbb{E}(X^2) = \\sum_{i=1}^{n} x_i^2 p_i\n\\]\n\n\n\nVarianza de una variable aleatoria continua\nSi $ X $ es una variable aleatoria continua con densidad $ f_X(x) $, entonces:\n\\[\n\\operatorname{Var}(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f_X(x)\\,dx\n\\]\ny de manera equivalente:\n\\[\n\\operatorname{Var}(X) = \\int_{-\\infty}^{\\infty} x^2 f_X(x)\\,dx - \\mu^2\n\\]\n\n\n\nDesviación estándar\nLa desviación estándar se define como la raíz cuadrada de la varianza:\n\\[\n\\sigma = \\sqrt{\\operatorname{Var}(X)}\n\\]\nEsta medida tiene la ventaja de expresarse en las mismas unidades que la variable original.\n\n\n\nPropiedades de la varianza\nSean $ X $ una variable aleatoria y $ a, b $. Entonces:\n\n\\[\n\\operatorname{Var}(aX + b) = a^2 \\operatorname{Var}(X)\n\\]\nSi $ X $ e $ Y $ son independientes: \\[\n\\operatorname{Var}(X + Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y)\n\\]",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#distribuciones-de-probabilidad-discretas",
    "href": "material/modulo_02/probabilidad.html#distribuciones-de-probabilidad-discretas",
    "title": "Elementos de Probabilidad",
    "section": "Distribuciones de Probabilidad Discretas",
    "text": "Distribuciones de Probabilidad Discretas\nLas distribuciones de probabilidad discretas describen el comportamiento de variables aleatorias que toman un número finito o numerable de valores.\n\n\nDistribución Bernoulli\nUna variable aleatoria $ X $ sigue una distribución Bernoulli si solo puede tomar dos valores:\n\\[\nX =\n\\begin{cases}\n1 & \\text{con probabilidad } p \\\\\n0 & \\text{con probabilidad } 1-p\n\\end{cases}\n\\]\nLa función de masa de probabilidad es:\n\\[\nP(X = x) = p^x (1-p)^{1-x}, \\quad x \\in \\{0,1\\}\n\\]\nValor esperado y varianza:\n\\[\n\\mathbb{E}(X) = p, \\quad \\operatorname{Var}(X) = p(1-p)\n\\]\n\n\n\nDistribución Binomial\nUna variable aleatoria $ X $ sigue una distribución binomial con parámetros $ n $ y $ p $ si representa el número de éxitos en $ n $ ensayos de Bernoulli independientes.\nLa función de probabilidad es:\n\\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0,1,\\dots,n\n\\]\nValor esperado y varianza:\n\\[\n\\mathbb{E}(X) = np, \\quad \\operatorname{Var}(X) = np(1-p)\n\\]\nSugerencia de imagen:\nGráfico de barras de una distribución binomial para distintos valores de $ p $.\n\n\n\nDistribución Poisson\nUna variable aleatoria $ X $ sigue una distribución Poisson con parámetro $ &gt; 0 $ si:\n\\[\nP(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}, \\quad k = 0,1,2,\\dots\n\\]\nEsta distribución se utiliza para modelar el número de ocurrencias de un evento en un intervalo fijo de tiempo o espacio.\nValor esperado y varianza:\n\\[\n\\mathbb{E}(X) = \\lambda, \\quad \\operatorname{Var}(X) = \\lambda\n\\]",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#distribuciones-de-probabilidad-continuas",
    "href": "material/modulo_02/probabilidad.html#distribuciones-de-probabilidad-continuas",
    "title": "Elementos de Probabilidad",
    "section": "Distribuciones de Probabilidad Continuas",
    "text": "Distribuciones de Probabilidad Continuas\nLas distribuciones continuas describen variables aleatorias cuyos valores pertenecen a intervalos de la recta real.\n\n\nDistribución Uniforme Continua\nUna variable aleatoria $ X $ tiene distribución uniforme en el intervalo $ [a,b] $ si su densidad es:\n\\[\nf_X(x) =\n\\begin{cases}\n\\frac{1}{b-a}, & a \\le x \\le b \\\\\n0, & \\text{en otro caso}\n\\end{cases}\n\\]\nValor esperado y varianza:\n\\[\n\\mathbb{E}(X) = \\frac{a+b}{2}, \\quad\n\\operatorname{Var}(X) = \\frac{(b-a)^2}{12}\n\\]\n\n\n\nDistribución Normal\nUna variable aleatoria $ X $ sigue una distribución normal con media $ $ y varianza $ ^2 $ si su función de densidad es:\n\\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\n\\]\nEsta distribución es ampliamente utilizada debido al Teorema Central del Límite.\nSugerencia de imagen:\nCurva normal con media $ $ y desviación estándar $ $.",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#teorema-central-del-límite",
    "href": "material/modulo_02/probabilidad.html#teorema-central-del-límite",
    "title": "Elementos de Probabilidad",
    "section": "Teorema Central del Límite",
    "text": "Teorema Central del Límite\nEl Teorema Central del Límite establece que, bajo condiciones generales, la suma (o promedio) de un número grande de variables aleatorias independientes e idénticamente distribuidas converge en distribución a una normal, independientemente de la distribución original.\nFormalmente, si $ X_1, X_2, , X_n $ son i.i.d. con media $ $ y varianza $ ^2 $, entonces:\n\\[\n\\frac{\\sum_{i=1}^{n} X_i - n\\mu}{\\sigma \\sqrt{n}}\n\\;\\xrightarrow{d}\\; \\mathcal{N}(0,1)\n\\]\nEste resultado justifica el uso extensivo de la distribución normal en estadística aplicada.",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#aproximaciones-de-distribuciones",
    "href": "material/modulo_02/probabilidad.html#aproximaciones-de-distribuciones",
    "title": "Elementos de Probabilidad",
    "section": "Aproximaciones de Distribuciones",
    "text": "Aproximaciones de Distribuciones\nEn la práctica, el cálculo exacto de probabilidades puede resultar complejo cuando el tamaño de la muestra es grande o cuando las expresiones analíticas son difíciles de manejar. En estos casos, es habitual recurrir a aproximaciones de distribuciones, las cuales permiten simplificar los cálculos manteniendo un buen nivel de precisión.\n\n\nAproximación normal a la binomial\nCuando una variable aleatoria $ X $ sigue una distribución binomial con parámetros $ n $ y $ p $, es posible aproximarla mediante una distribución normal siempre que se cumplan las siguientes condiciones:\n\\[\nnp \\ge 5 \\quad \\text{y} \\quad n(1-p) \\ge 5\n\\]\nBajo estas condiciones, se tiene:\n\\[\nX \\sim \\text{Binomial}(n,p)\n\\quad \\approx \\quad\nY \\sim \\mathcal{N}(np,\\, np(1-p))\n\\]\nPara mejorar la aproximación, se utiliza la corrección por continuidad, evaluando probabilidades del tipo:\n\\[\nP(a \\le X \\le b) \\approx P(a - 0{,}5 \\le Y \\le b + 0{,}5)\n\\]\nSugerencia de imagen:\nSuperposición de un histograma binomial con la curva normal aproximada.\n\n\n\nAproximación de Poisson a la binomial\nCuando $ n $ es grande y $ p $ es pequeño, de modo que $ = np $ se mantiene constante, la distribución binomial puede aproximarse mediante una distribución Poisson:\n\\[\nX \\sim \\text{Binomial}(n,p)\n\\quad \\approx \\quad\n\\text{Poisson}(\\lambda = np)\n\\]\nEsta aproximación es especialmente útil en problemas de conteo de eventos raros.",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#funciones-de-transformación-de-variables-aleatorias",
    "href": "material/modulo_02/probabilidad.html#funciones-de-transformación-de-variables-aleatorias",
    "title": "Elementos de Probabilidad",
    "section": "Funciones de Transformación de Variables Aleatorias",
    "text": "Funciones de Transformación de Variables Aleatorias\nEn muchos problemas prácticos, el interés no está directamente en la variable aleatoria original, sino en una transformación de ella.\nSea $ Y = g(X) $, donde $ X $ es una variable aleatoria.\n\n\nTransformación de variables discretas\nSi $ X $ es discreta y toma valores $ x_1, x_2, $, entonces la distribución de $ Y $ se obtiene calculando:\n\\[\nP(Y = y) = \\sum_{x_i : g(x_i) = y} P(X = x_i)\n\\]\n\n\n\nTransformación de variables continuas\nSi $ X $ es continua y $ Y = g(X) $ es una función monótona, entonces la densidad de $ Y $ se obtiene mediante:\n\\[\nf_Y(y) = f_X\\big(g^{-1}(y)\\big)\\left| \\frac{d}{dy} g^{-1}(y) \\right|\n\\]\nSugerencia de imagen:\nTransformación gráfica de una densidad original a una nueva densidad mediante cambio de variable.",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#distribución-de-la-media-muestral",
    "href": "material/modulo_02/probabilidad.html#distribución-de-la-media-muestral",
    "title": "Elementos de Probabilidad",
    "section": "Distribución de la Media Muestral",
    "text": "Distribución de la Media Muestral\nSea $ X_1, X_2, , X_n $ una muestra aleatoria de una población con media $ $ y varianza $ ^2 $. La media muestral se define como:\n\\[\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n\\]\nLas propiedades de la media muestral son:\n\\[\n\\mathbb{E}(\\bar{X}) = \\mu\n\\]\n\\[\n\\operatorname{Var}(\\bar{X}) = \\frac{\\sigma^2}{n}\n\\]\nEn virtud del Teorema Central del Límite, para tamaños muestrales grandes se cumple:\n\\[\n\\bar{X} \\approx \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#distribución-de-la-varianza-muestral",
    "href": "material/modulo_02/probabilidad.html#distribución-de-la-varianza-muestral",
    "title": "Elementos de Probabilidad",
    "section": "Distribución de la Varianza Muestral",
    "text": "Distribución de la Varianza Muestral\nLa varianza muestral se define como:\n\\[\nS^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\n\\]\nSi la población original es normal, entonces la variable:\n\\[\n\\frac{(n-1)S^2}{\\sigma^2}\n\\]\nsigue una distribución chi-cuadrado con $ n-1 $ grados de libertad.",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#distribución-t-de-student",
    "href": "material/modulo_02/probabilidad.html#distribución-t-de-student",
    "title": "Elementos de Probabilidad",
    "section": "Distribución t de Student",
    "text": "Distribución t de Student\nCuando la varianza poblacional $ ^2 $ es desconocida y el tamaño de muestra es pequeño, se utiliza la distribución t de Student.\nLa variable:\n\\[\nT = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}}\n\\]\nsigue una distribución t con $ n-1 $ grados de libertad.\nSugerencia de imagen:\nComparación gráfica entre la distribución normal estándar y distribuciones t con distintos grados de libertad.",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_02/probabilidad.html#distribución-f-de-fishersnedecor",
    "href": "material/modulo_02/probabilidad.html#distribución-f-de-fishersnedecor",
    "title": "Elementos de Probabilidad",
    "section": "Distribución F de Fisher–Snedecor",
    "text": "Distribución F de Fisher–Snedecor\nLa distribución F surge al considerar el cociente de dos variables chi-cuadrado independientes, cada una dividida por sus respectivos grados de libertad.\nSi:\n\\[\nU_1 \\sim \\chi^2(\\nu_1), \\quad U_2 \\sim \\chi^2(\\nu_2)\n\\]\nentonces:\n\\[\nF = \\frac{U_1/\\nu_1}{U_2/\\nu_2}\n\\]\nsigue una distribución F con $ (_1, _2) $ grados de libertad.\nEsta distribución se utiliza principalmente en el análisis de varianzas (ANOVA).",
    "crumbs": [
      "Teoría de Probabilidad",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elementos de Probabilidad</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/intervalos.html",
    "href": "material/modulo_03/intervalos.html",
    "title": "Inferencia Estadística",
    "section": "",
    "text": "Introducción\nComo se ha podido apreciar en los módulos anteriores, la estadística trata con la recolección de datos, su análisis e interpretación.\nEn inferencia clásica y teoría de decisiones, las observaciones se postulan como variables aleatorias. La ley o distribución de la(s) variable(s) aleatoria(s) observable(s), denotada por $ P $, se asume perteneciente a una familia paramétrica conocida en su forma general, pero con parámetros desconocidos.\nUn objetivo fundamental de la inferencia estadística es determinar valores factibles de dichos parámetros a partir de los datos.\nLa utilidad de los datos, generados a partir de muestras probabilísticas, es inferir características esenciales de la población a partir de la muestra.\nUna de las áreas centrales de la inferencia estadística es la estimación de parámetros, para lo cual se requieren algunas definiciones básicas.\nDefinición 7.1 (Parámetro).\nEs una característica numérica de la distribución de la población que describe, parcial o completamente, la función de probabilidad de la característica de interés. Habitualmente se simboliza por la letra griega $ $.\nDefinición 7.2 (Espacio Paramétrico).\nEs el conjunto de posibles valores que puede tomar el(los) parámetro(s). Se simboliza por la letra griega mayúscula $ $.\nFigura sugerida 7.1. Esquema del problema de estimación: población → muestra → estadístico → estimador del parámetro.\nLos posibles valores de la muestra aleatoria constituyen el espacio de información, y utilizando algún resumen apropiado (estadística), se construye un estimador del(los) parámetro(s) asociado(s) a la familia de distribución supuesta.\nSe distinguen dos métodos principales de estimación: - Estimación puntual, que entrega un único valor numérico para el parámetro. - Estimación por intervalo, que entrega un conjunto de valores plausibles para el parámetro.",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferencia Estadística</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/intervalos.html#método-de-momentos",
    "href": "material/modulo_03/intervalos.html#método-de-momentos",
    "title": "Inferencia Estadística",
    "section": "7.2 Método de Momentos",
    "text": "7.2 Método de Momentos\nEs uno de los métodos más antiguos de estimación puntual. Consiste en igualar los momentos poblacionales con los correspondientes momentos muestrales. Este método genera tantas ecuaciones como parámetros se deseen estimar.\nDefinición 7.3 (Momentos muestrales).\nSean $ X_1, X_2, , X_n $ una muestra aleatoria. El r-ésimo momento muestral respecto del origen se define como:\n\\[\nM_r = \\frac{1}{n} \\sum_{i=1}^{n} X_i^r\n\\]\nPara $ r = 1 $ se obtiene la media muestral; para $ r = 2,3,4 $, se obtienen medidas asociadas a variabilidad y forma.\nDefinición 7.4 (Momentos poblacionales).\nSea $ X $ una variable aleatoria con función de probabilidad $ f(x;) $. El r-ésimo momento poblacional se define como:\n\\[\n\\mu_r = \\mathbb{E}[X^r]\n\\]\n\n\nAplicación 7.1\nSea $ X_1, X_2, , X_n $ una muestra aleatoria de una población con distribución Bernoulli:\n\\[\n\\mathbb{P}(X = x) = p^x(1-p)^{1-x}, \\quad x = 0,1\n\\]\nEl primer momento poblacional es:\n\\[\n\\mathbb{E}[X] = p\n\\]\nIgualando con el primer momento muestral:\n\\[\n\\hat{p} = \\bar{X}\n\\]\n\n\n\nAplicación 7.2\nSuponga que el tiempo de vida (en años) de un componente eléctrico tiene densidad:\n\\[\nf(x;\\alpha) = \\frac{3}{\\alpha^3}x^2, \\quad 0 &lt; x &lt; \\alpha\n\\]\nEl primer momento poblacional es:\n\\[\n\\mathbb{E}[X] = \\frac{3}{4}\\alpha\n\\]\nIgualando con la media muestral:\n\\[\n\\hat{\\alpha} = \\frac{4}{3}\\bar{X}\n\\]\n\n\n\nAplicación 7.3\nSea $ X_1, , X_n $ una muestra de una población Exponencial con parámetro $ $:\n\\[\nf(x;\\theta) = \\frac{1}{\\theta}e^{-x/\\theta}, \\quad x&gt;0\n\\]\nComo:\n\\[\n\\mathbb{E}[X] = \\theta\n\\]\nSe obtiene el estimador de momentos:\n\\[\n\\hat{\\theta} = \\bar{X}\n\\]",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferencia Estadística</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/intervalos.html#método-de-máxima-verosimilitud",
    "href": "material/modulo_03/intervalos.html#método-de-máxima-verosimilitud",
    "title": "Inferencia Estadística",
    "section": "7.3 Método de Máxima Verosimilitud",
    "text": "7.3 Método de Máxima Verosimilitud\nEste método selecciona como estimador el valor del parámetro que maximiza la probabilidad de observar la muestra.\nDefinición 7.5 (Función de Verosimilitud).\nSea $ X_1, , X_n $ una muestra aleatoria con densidad $ f(x;) $. La verosimilitud es:\n\\[\nL(\\theta;\\mathbf{x}) = \\prod_{i=1}^{n} f(x_i;\\theta)\n\\]\nPara facilitar los cálculos se utiliza la log-verosimilitud:\n\\[\n\\ell(\\theta;\\mathbf{x}) = \\ln L(\\theta;\\mathbf{x})\n\\]\n\n\nAplicación 7.4\nSea $ X_1, , X_n $ una muestra con densidad:\n\\[\nf(x;\\phi) = \\frac{1}{4} e^{-(x-\\phi)/4}, \\quad x&gt;\\phi\n\\]\nLa log-verosimilitud es:\n\\[\n\\ell(\\phi) = -n\\ln 4 - \\sum_{i=1}^{n}\\frac{x_i-\\phi}{4}\n\\]\nDerivando e igualando a cero:\n\\[\n\\hat{\\phi}_{MV} = \\bar{X} - 4\n\\]",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferencia Estadística</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/intervalos.html#propiedades-de-los-estimadores",
    "href": "material/modulo_03/intervalos.html#propiedades-de-los-estimadores",
    "title": "Inferencia Estadística",
    "section": "7.4 Propiedades de los Estimadores",
    "text": "7.4 Propiedades de los Estimadores\nSea $ T = T(X_1,,X_n) $ una estadística.\n\nEstimadores Insesgados\nDefinición 7.6 (Insesgamiento).\nUn estimador $ T $ de $ $ es insesgado si:\n\\[\n\\mathbb{E}[T] = \\theta\n\\]\n\n\n\nError Cuadrático Medio\nDefinición 7.7.\n\\[\nECM(T) = \\mathbb{E}[(T-\\theta)^2]\n\\]\nSe puede descomponer como:\n\\[\nECM(T) = \\operatorname{Var}(T) + (\\mathbb{E}[T]-\\theta)^2\n\\]\n\n\n\nEficiencia\nDefinición 7.8 (Eficiencia relativa).\n\\[\nEf(T_1,T_2) = \\frac{ECM(T_2)}{ECM(T_1)}\n\\]\n\n\n\nConsistencia\nDefinición 7.9 (Consistencia en media cuadrática).\n\\[\n\\lim_{n\\to\\infty} ECM(T_n) = 0\n\\]",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferencia Estadística</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/intervalos.html#estimación-por-intervalo",
    "href": "material/modulo_03/intervalos.html#estimación-por-intervalo",
    "title": "Inferencia Estadística",
    "section": "7.5 Estimación por Intervalo",
    "text": "7.5 Estimación por Intervalo\nLa estimación puntual no entrega información sobre la precisión del estimador. Los intervalos de confianza solucionan este problema.\nDefinición 7.10.\nUn intervalo $ [T_1, T_2] $ es un intervalo de confianza para $ $ con nivel $ 100% $ si:\n\\[\n\\mathbb{P}(T_1 \\le \\theta \\le T_2) = \\gamma\n\\]\n\n\nIntervalo de Confianza para la Media (σ conocida)\n\\[\nIC(\\mu) = \\left[\\bar{X} \\pm Z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right]\n\\]\n\n\nIntervalo de Confianza para la Media (σ desconocida)\n\\[\nIC(\\mu) = \\left[\\bar{X} \\pm t_{1-\\alpha/2,n-1}\\frac{S}{\\sqrt{n}}\\right]\n\\]\n\n\n\nIntervalo de Confianza para una Proporción\n\\[\nIC(p) = \\left[\\hat{p} \\pm Z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right]\n\\]\n\n\n\nIntervalo de Confianza para la Varianza\n\\[\nIC(\\sigma^2) =\n\\left[\n\\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2,n-1}},\n\\frac{(n-1)S^2}{\\chi^2_{\\alpha/2,n-1}}\n\\right]\n\\]\n\nFigura sugerida final: Curvas Normal, t-Student y Chi-cuadrado ilustrando regiones de confianza.",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferencia Estadística</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/intervalos.html#intervalos-unilaterales-de-confianza",
    "href": "material/modulo_03/intervalos.html#intervalos-unilaterales-de-confianza",
    "title": "Inferencia Estadística",
    "section": "Intervalos Unilaterales de Confianza",
    "text": "Intervalos Unilaterales de Confianza\nEn algunos problemas prácticos no interesa estimar un intervalo bilateral, sino establecer una cota superior o inferior para el parámetro poblacional.\n\nIntervalo Unilateral Superior para la Media\nSe desea encontrar un valor $ k $ tal que:\n\\[\n\\mathbb{P}(\\mu \\le k) = 1-\\alpha\n\\]\nPartiendo de la cantidad pivotal:\n\\[\nZ = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\n\\]\nSe tiene:\n\\[\n\\mathbb{P}\\left( \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\ge -Z_{1-\\alpha} \\right)=1-\\alpha\n\\]\nLo que conduce a:\n\\[\n\\mathbb{P}\\left(\\mu \\le \\bar{X}+Z_{1-\\alpha}\\frac{\\sigma}{\\sqrt{n}}\\right)=1-\\alpha\n\\]\nPor tanto, el intervalo unilateral superior es:\n\\[\nIC_U(\\mu) = \\left(-\\infty,\\; \\bar{X}+Z_{1-\\alpha}\\frac{\\sigma}{\\sqrt{n}}\\right]\n\\]\nSi la varianza poblacional es desconocida, se reemplaza $ Z_{1-} $ por $ t_{1-,n-1} $ y $ $ por $ S $.",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferencia Estadística</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/intervalos.html#aplicación-7.15-la-decisión-att-o-sprint",
    "href": "material/modulo_03/intervalos.html#aplicación-7.15-la-decisión-att-o-sprint",
    "title": "Inferencia Estadística",
    "section": "Aplicación 7.15: La decisión: AT&T o Sprint",
    "text": "Aplicación 7.15: La decisión: AT&T o Sprint\nUn contador debe decidir entre dos compañías telefónicas para llamadas de larga distancia. Se obtiene la siguiente información:\n\n\n\n\n\n\n\n\n\nCompañía\nNúmero de llamadas\nMedia (US\\() | Desviación estándar (US\\))\n\n\n\n\n\nAT&T\n145\n4.07\n0.97\n\n\nSprint\n102\n3.89\n0.85\n\n\n\nSe definen las variables aleatorias:\n\n$ X $: Costo de llamadas en AT&T, $ X N(_x,_x^2) $\n$ Y $: Costo de llamadas en Sprint, $ Y N(_y,_y^2) $\n\n\nIntervalos de confianza del 96% para la media\n\\[\nIC_{96\\%}(\\mu_x)=\n\\left[\\bar{x}\\pm Z_{0.98}\\frac{s_x}{\\sqrt{n_x}}\\right]\n= [3.90;\\,4.42]\n\\]\nInterpretación:\nCon un 96% de confianza, el verdadero costo medio de llamadas en AT&T se encuentra entre US$ 3.90 y US$ 4.42.\n\\[\nIC_{96\\%}(\\mu_y)=\n\\left[\\bar{y}\\pm Z_{0.98}\\frac{s_y}{\\sqrt{n_y}}\\right]\n= [3.72;\\,4.31]\n\\]\nInterpretación:\nCon un 96% de confianza, el verdadero costo medio de llamadas en Sprint se encuentra entre US$ 3.72 y US$ 4.31.\n\n\n\nIntervalos de confianza del 96% para la varianza\n\\[\nIC_{96\\%}(\\sigma_x^2)=\n\\left[\n\\frac{(n_x-1)s_x^2}{\\chi^2_{0.98,n_x-1}},\n\\frac{(n_x-1)s_x^2}{\\chi^2_{0.02,n_x-1}}\n\\right]\n= [0.71;\\,1.17]\n\\]\nInterpretación:\nCon un 96% de confianza, la varianza del costo en AT&T se encuentra entre 0.71 y 1.17 US$².\n\\[\nIC_{96\\%}(\\sigma_y^2)=\n\\left[\n\\frac{(n_y-1)s_y^2}{\\chi^2_{0.98,n_y-1}},\n\\frac{(n_y-1)s_y^2}{\\chi^2_{0.02,n_y-1}}\n\\right]\n= [0.51;\\,0.93]\n\\]\nInterpretación:\nCon un 96% de confianza, la varianza del costo en Sprint se encuentra entre 0.51 y 0.93 US$².\n\n\n\nIntervalo Unilateral del 98% para la Media de AT&T\nSe desea una cota superior para el verdadero costo medio de AT&T.\n\\[\n\\mathbb{P}(\\mu_x \\le k)=0.98\n\\]\nUsando la distribución normal:\n\\[\nk=\\bar{X}+Z_{0.98}\\frac{s_x}{\\sqrt{n_x}}\n\\]\nPor tanto, el intervalo unilateral es:\n\\[\nIC_{98\\%}(\\mu_x)=(-\\infty;\\,4.42]\n\\]\nInterpretación:\nCon un 98% de confianza, el costo medio real de llamadas en AT&T no supera US$ 4.42. En términos prácticos, el costo no puede ser negativo.\n\nFigura sugerida final:\nComparación gráfica de intervalos de confianza para AT&T y Sprint (medias y dispersión), mostrando solapamiento y rangos de decisión.",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferencia Estadística</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/tests.html",
    "href": "material/modulo_03/tests.html",
    "title": "Prueba de Hipótesis",
    "section": "",
    "text": "Introducción a la Prueba de Hipótesis\nUn aspecto fundamental dentro de la inferencia estadística es el que denominamos Prueba de Hipótesis, también llamado Contraste de Hipótesis o Dócima de Hipótesis.\nEn la actualidad, los sociólogos han llegado a denominar a esta época como la sociedad del riesgo. Constantemente debemos decidir entre posibilidades excluyentes y, por lo tanto, asumir el riesgo asociado a nuestras decisiones. Por ejemplo, al comprar un activo financiero, debemos decidir cuál adquirir dentro de un conjunto de posibilidades y, posteriormente, elegir el método de depreciación a utilizar. Estas decisiones implican consecuencias futuras que pueden derivar en ascensos o despidos.\nEste riesgo, en la mayoría de los casos, es completamente subjetivo e imposible de cuantificar con exactitud, en particular en decisiones íntimas o existenciales. ¿Cómo medir dicho riesgo? No existe una respuesta única y concluyente.\nPor lo general, la decisión a tomar se da entre un conjunto de resultados —también llamados estados de la naturaleza— desconocidos para el decisor. Aunque existen técnicas para abordar estos problemas, las Pruebas de Hipótesis que estudiaremos estarán limitadas a solo dos estados de la naturaleza posibles, mutuamente excluyentes: ocurre el estado A o no ocurre (en cuyo caso ocurre el estado B).\nLa toma de decisiones es una realidad frecuente en las organizaciones, donde a menudo se requiere decidir casi en tiempo real. Esta exigencia dificulta un análisis exhaustivo; sin embargo, con un esfuerzo adicional razonable, dichas decisiones pueden estar respaldadas por procedimientos estadísticos de alto nivel.\nEl desarrollo y análisis de una prueba de hipótesis sigue un procedimiento similar al utilizado en los Intervalos de Confianza. La diferencia principal radica en que, en intervalos de confianza, se desconoce el valor del parámetro poblacional de interés y se desea estimarlo a partir de una muestra aleatoria. En cambio, en la prueba de hipótesis existe una conjetura previa sobre dicho parámetro, la cual se contrasta con la evidencia muestral.\nEn una prueba de hipótesis se decide aceptar o rechazar un determinado estado de la naturaleza. Tradicionalmente, la conjetura principal se denomina Hipótesis Nula, simbolizada por \\(H_0\\), mientras que la alternativa se denomina Hipótesis Alternativa, simbolizada por \\(H_1\\).\nAl tomar una decisión entre dos estados mutuamente excluyentes, existe la posibilidad de cometer errores. Estos errores se clasifican de la siguiente forma (ver Figura 1):\nSugerencia de imagen: diagrama de matriz de decisión mostrando decisiones correctas y errores Tipo I y Tipo II.",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prueba de Hipótesis</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/tests.html#introducción-a-la-prueba-de-hipótesis",
    "href": "material/modulo_03/tests.html#introducción-a-la-prueba-de-hipótesis",
    "title": "Prueba de Hipótesis",
    "section": "",
    "text": "Tipos de errores en pruebas de hipótesis\n\n\n\nEstado real / Decisión\nNo se rechaza \\(H_0\\)\nSe rechaza \\(H_0\\)\n\n\n\n\n\\(H_0\\) verdadera\nDecisión correcta\nError Tipo I\n\n\n\\(H_0\\) falsa\nError Tipo II\nDecisión correcta\n\n\n\nFigura 1: Tipos de errores en decisiones excluyentes.\n\nError Tipo I (\\(\\alpha\\)): rechazar la hipótesis nula cuando en realidad es verdadera.\nError Tipo II (\\(\\beta\\)): no rechazar la hipótesis nula cuando en realidad es falsa.\n\nEl error Tipo I, \\(\\alpha\\), coincide con el nivel de significancia utilizado en intervalos de confianza y es el error que el experimentador controla. En muchos contextos, se considera menos dañino que el error Tipo II, aunque esta apreciación depende del problema.\nPor ejemplo, en un juicio, la hipótesis nula es que una persona es inocente. El error Tipo I implica condenar a un inocente, mientras que el error Tipo II implica absolver a un culpable. En este contexto, suele considerarse preferible cometer un error Tipo II.\nNo rechazar una hipótesis implica únicamente que los datos no proporcionan evidencia suficiente para refutarla; no implica que sea verdadera. Rechazar una hipótesis indica que la evidencia muestral es suficiente para descartarla, aunque no prueba con certeza absoluta que sea falsa.\nDesde el punto de vista probabilístico: - \\(\\alpha = \\mathbb{P}(\\text{Rechazar } H_0 \\mid H_0 \\text{ verdadera})\\) - \\(\\beta = \\mathbb{P}(\\text{No rechazar } H_0 \\mid H_0 \\text{ falsa})\\)\nEl error Tipo II no puede cuantificarse sin especificar un valor concreto bajo la hipótesis alternativa. Para abordar este problema se introduce la función de potencia.\n\n\nFunción de Potencia\nLa función de potencia de una prueba es la probabilidad de rechazar la hipótesis nula cuando la hipótesis alternativa es verdadera:\n\\[\n\\pi(\\theta) = \\mathbb{P}(\\text{Rechazar } H_0 \\mid H_1 \\text{ verdadera}) = 1 - \\beta\n\\]\nPara un valor específico del parámetro alternativo, se habla simplemente de la potencia de la prueba.\nLa forma de la función de potencia depende de cómo se formule la hipótesis alternativa (unilateral o bilateral).\nEn general, se asume que la muestra proviene de una distribución Normal o que el tamaño muestral es suficientemente grande (\\(n &gt; 30\\)) para aplicar el Teorema del Límite Central.\n\n\nTipos de hipótesis sobre la media\nHipótesis simples: \\[\nH_0: \\mu = \\mu_0 \\quad \\text{vs} \\quad H_1: \\mu = \\mu_1 \\quad (\\mu_0 &lt; \\mu_1)\n\\]\nHipótesis compuestas (unilaterales): \\[\nH_0: \\mu = \\mu_0 \\quad \\text{vs} \\quad H_1: \\mu &gt; \\mu_0\n\\]\n\\[\nH_0: \\mu = \\mu_0 \\quad \\text{vs} \\quad H_1: \\mu &lt; \\mu_0\n\\]\nHipótesis bilaterales: \\[\nH_0: \\mu = \\mu_0 \\quad \\text{vs} \\quad H_1: \\mu \\neq \\mu_0\n\\]\nEn los casos compuestos, la función de potencia permite evaluar el desempeño de la prueba para distintos valores del parámetro alternativo.",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prueba de Hipótesis</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/tests.html#ejemplo-1-proporción-de-mercado",
    "href": "material/modulo_03/tests.html#ejemplo-1-proporción-de-mercado",
    "title": "Prueba de Hipótesis",
    "section": "Ejemplo 1: Proporción de mercado",
    "text": "Ejemplo 1: Proporción de mercado\nSe cree que una compañía cubre el 40% de la demanda de un producto. Se toma una muestra aleatoria de 18 consumidores y se define como región crítica: \\[\nW = \\{X \\leq 3 \\ \\text{o} \\ X \\geq 12\\}\n\\] donde \\(X\\) es el número de consumidores que compran a la compañía.\n\\[\nX \\sim \\text{Binomial}(18, p)\n\\] \\[\nH_0: p = 0.4 \\quad \\text{vs} \\quad H_1: p \\neq 0.4\n\\]\n\nError Tipo I\n\\[\n\\alpha = \\mathbb{P}(X \\leq 3 \\ \\text{o} \\ X \\geq 12 \\mid p = 0.4)\n\\]\n\n\nFunción de potencia\n\\[\n\\pi(p) = \\mathbb{P}(X \\leq 3 \\ \\text{o} \\ X \\geq 12 \\mid p)\n\\]\nTabla 1: Valores para la función de potencia\n\n\n\n\\(p\\)\n\\(\\pi(p)\\)\n\n\n\n\n0.9\n—\n\n\n0.8\n—\n\n\n0.6\n—\n\n\n0.4\n—\n\n\n0.1\n—\n\n\n\nSugerencia de imagen: gráfico de la función de potencia \\(\\pi(p)\\) en función de \\(p\\).",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prueba de Hipótesis</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/tests.html#aplicación-8.1-rendimiento-de-neumáticos",
    "href": "material/modulo_03/tests.html#aplicación-8.1-rendimiento-de-neumáticos",
    "title": "Prueba de Hipótesis",
    "section": "Aplicación 8.1: Rendimiento de neumáticos",
    "text": "Aplicación 8.1: Rendimiento de neumáticos\nVariables - \\(X\\): duración (MKM) neumáticos Tipo A - \\(Y\\): duración (MKM) neumáticos Tipo B\nSupuestos \\[\nX \\sim N(\\mu_x, \\sigma_x^2), \\quad Y \\sim N(\\mu_y, \\sigma_y^2)\n\\]\nDatos\n\n\n\nTipo\n\\(\\bar{x}\\)\n\\(s^2\\)\n\\(n\\)\n\n\n\n\nA\n28.10\n3.26\n51\n\n\nB\n27.09\n3.63\n101\n\n\n\n\nContraste sobre la media\n\\[\nH_0: \\mu_x = 26.5 \\quad \\text{vs} \\quad H_1: \\mu_x &gt; 26.5\n\\]\nEstadístico: \\[\nZ = \\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}}\n\\]\nRegión crítica: \\[\nZ &gt; z_{1-\\alpha}\n\\]\nConclusión: se rechaza \\(H_0\\) para ambas marcas.",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prueba de Hipótesis</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/tests.html#aplicación-8.2-límites-de-velocidad",
    "href": "material/modulo_03/tests.html#aplicación-8.2-límites-de-velocidad",
    "title": "Prueba de Hipótesis",
    "section": "Aplicación 8.2: Límites de velocidad",
    "text": "Aplicación 8.2: Límites de velocidad\n\\[\nH_0: \\mu = 90 \\quad \\text{vs} \\quad H_1: \\mu &lt; 90\n\\]\nEstadístico t: \\[\nT = \\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}}\n\\]\nConclusión: no se rechaza \\(H_0\\).",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prueba de Hipótesis</span>"
    ]
  },
  {
    "objectID": "material/modulo_03/tests.html#aplicación-8.3-consumo-de-marihuana",
    "href": "material/modulo_03/tests.html#aplicación-8.3-consumo-de-marihuana",
    "title": "Prueba de Hipótesis",
    "section": "Aplicación 8.3: Consumo de marihuana",
    "text": "Aplicación 8.3: Consumo de marihuana\n\\[\nH_0: \\mu = 15 \\quad \\text{vs} \\quad H_1: \\mu &gt; 15\n\\]\n\\[\nH_0: \\sigma^2 = 20 \\quad \\text{vs} \\quad H_1: \\sigma^2 &gt; 20\n\\]\nConclusión: no se rechaza \\(H_0\\) en ambos casos.\nSugerencia de imagen: distribución t y chi-cuadrado con regiones críticas resaltadas.",
    "crumbs": [
      "Inferencia estadística",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prueba de Hipótesis</span>"
    ]
  }
]